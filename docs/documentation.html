<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>scm_confocal API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>scm_confocal</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">__version__ = &#39;1.1&#39;

from .sp8 import sp8_series
from .visitech import visitech_series,visitech_faststack
from .util import util

#make available when using &#39;from scm_confocal import *&#39;
__all__ = [
    &#39;sp8_series&#39;,
    &#39;visitech_series&#39;,
    &#39;visitech_faststack&#39;,
    &#39;util&#39;
]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="scm_confocal.sp8" href="sp8.html">scm_confocal.sp8</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
<dt><code class="name"><a title="scm_confocal.visitech" href="visitech.html">scm_confocal.visitech</a></code></dt>
<dd>
<section class="desc"></section>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="scm_confocal.sp8_series"><code class="flex name class">
<span>class <span class="ident">sp8_series</span></span>
<span>(</span><span>fmt='*.tif')</span>
</code></dt>
<dd>
<section class="desc"><p>Class of functions related to the sp8 microscope. The functions assume that
the data are exported as .tif files and placed in a own folder per series.
The current working directory is assumed to be that folder. For several
functions it is required that the xml metadata is present in a subfolder of
the working directory called 'MetaData', which is normally generated
automatically when exporting tif files as raw data.</p>
<p>Initialize the class instance and assign the filenames of the data.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>fmt</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>format to use for finding the files. Uses the notation of the glob
library. The default is '*.tif'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class sp8_series:
    &#34;&#34;&#34;
    Class of functions related to the sp8 microscope. The functions assume that
    the data are exported as .tif files and placed in a own folder per series.
    The current working directory is assumed to be that folder. For several
    functions it is required that the xml metadata is present in a subfolder of
    the working directory called &#39;MetaData&#39;, which is normally generated
    automatically when exporting tif files as raw data.
    &#34;&#34;&#34;
    
    def __init__(self,fmt=&#39;*.tif&#39;):
        &#34;&#34;&#34;
        Initialize the class instance and assign the filenames of the data.

        Parameters
        ----------
        fmt : str, optional
            format to use for finding the files. Uses the notation of the glob
            library. The default is &#39;*.tif&#39;.


        Returns
        -------
        None.

        &#34;&#34;&#34;

        self.filenames = glob.glob(fmt)
        if len(self.filenames) &lt; 1:
            raise ValueError(&#39;No images found in current directory&#39;)
        
    def load_data(self, filenames=None, first=None, last=None, dtype=np.uint8):
        &#34;&#34;&#34;
        Loads the sequence of images into ndarray of form (files,y,x) and
        converts the data to dtype

        Parameters
        ----------
        filenames : list of str, optional
            filenames of images to load. The default is what is passed from 
            __init__, which by default is all .tif images in the current
            working directory.
        first : None or int, optional
            index of first image to load. The default is None.
        last : None or int, optional
            index of last image to load plus one. The default is None.
        dtype : (numpy) datatype, optional
            type to scale data to. The default is np.uint8.

        Returns
        -------
        data : numpy.ndarray
            3d numpy array with dimension order (filenames,y,x).

        &#34;&#34;&#34;
        import PIL.Image
        
        if filenames == None:
            filenames = sorted(self.filenames)
        
        data = np.array([np.array(PIL.Image.open(name)) for name in filenames[first:last]])

        #check if images are 2D (i.e. greyscale)
        if data.ndim &gt; 3:
            print(&#34;[WARNING] sp8_series.load_data(): images do not have the correct dimensionality, &#34;+
                  &#34;did you load colour images perhaps? Continueing with average values of higher dimensions&#34;)
            data = np.mean(data,axis=tuple(range(3,data.ndim)),dtype=dtype)


        if not data.dtype == dtype:
            data = data/np.amax(data)*np.iinfo(dtype).max
            data = data.astype(dtype)
        self.data = data
        return data
    
    def load_stack(self,dim_range={},dtype=np.uint8):
        &#34;&#34;&#34;
        Similar to sp8_series.load_data(), but converts the 3D array of images
        automatically to a np.ndarray of the appropriate dimensionality.
        
        Array dimensions are specified as follows:
            - If the number of detector channels is 2 or higher, the first
              array axis is the detector channel index (named &#39;channel&#39;).
            - If the number of channels is 1, the first array axis is the first
              available dimension (instead of &#39;channel&#39;).
            - Each subsequent array axis corresponds to a dimension as
              specified by and in reversed order of the metadata exported by
              the microscope software, excluding dimensions which are not
              available. The default order of dimensions in the metadata is:
                 - (0 = &#39;channel&#39;)
                 -  1 = &#39;x-axis&#39;
                 -  2 = &#39;y-axis&#39;
                 -  3 = &#39;z-axis&#39;
                 -  4 = &#39;time&#39;
                 -  5 = &#39;detection wavelength&#39;
                 -  6 = &#39;excitation wavelength&#39;
        
            - As an example, a 2 channel xyt measurement would result in a 4-d
              array with axis order (&#39;channel&#39;,&#39;time&#39;,&#39;y-axis&#39;,
              &#39;x-axis&#39;), and a single channel xyz scan would be returned as
              (&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;)
        
        For loading only part of the total dataset, the dim_range parameter can
        be used to specify a range along any of the dimensions. This will be
        more memory efficient than loading the entire stack and then discarding
        part of the data. For slicing along the x or y axis this is not
        possible and whole (xy) images must be loaded prior to discarding
        data outside the specified x or y axis range.

        Parameters
        ----------
        dim_range : dict, optional
            dict, with keys corresponding to channel/dimension labels as above
            and slice objects as values. This allows you to only load part of
            the data along any of the dimensions, such as only loading one
            channel of multichannel data or a particular z-range. An example
            use for only taking time steps up to 5 and z-slice 20 to 30 would
            be:
                dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
            The default is {}.
        dtype : (numpy) datatype, optional
            type to scale data to. The default is np.uint8.

        Returns
        -------
        data : numpy.ndarray
            ndarray with the pixel values
        dimorder : tuple
            tuple with lenght data.ndim specifying the ordering of dimensions
            in the data with labels from the metadata of the microscope.
        &#34;&#34;&#34;

        #load the metadata
        try:
            channels = self.metadata_channels
        except AttributeError:
            channels = sp8_series.get_metadata_channels(self)
        try:
            dimensions = self.metadata_dimensions
        except AttributeError:
            dimensions = sp8_series.get_metadata_dimensions(self)
        
        #determine what the new shape should be from dimensional metadata
        newshape = [int(dim[&#39;NumberOfElements&#39;]) for dim in reversed(dimensions)]
        
        #replace dimID with more sensible label
        def DimIDreplace(idlist):
            pattern = zip(list(&#39;123456&#39;),[&#39;x-axis&#39;,&#39;y-axis&#39;,&#39;z-axis&#39;,&#39;time&#39;,
                          &#39;detection wavelength&#39;,&#39;emission wavelength&#39;])
            for l,r in pattern:
                idlist = idlist.replace(l,r)
            return idlist
        
        order = [DimIDreplace(dim[&#39;DimID&#39;]) for dim in reversed(dimensions)]
        
        #append channel (but before x and y) information for multichannel data
        if len(channels)&gt;1:
            newshape = newshape[:-2] + [len(channels)] + newshape[-2:]
            order = order[:-2] + [&#39;channel&#39;] + order[-2:]
        
        #load filenames
        filenames = self.filenames

        #apply slicing to the list of filenames before loading images
        if len(dim_range) &gt; 0:

            self._stack_dim_range = dim_range

            #give a warning that only whole xy images are loaded
            if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
                print(&#34;[WARNING] confocal.sp8_series.load_stack: Loading only&#34;+
                      &#34; part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
                      &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
                      &#34;into memory before discarding values outside of the &#34;+
                      &#34;slice range specified for the x-axis and/or y-axis. &#34;+
                      &#34;Other axes for which a range is specified will still &#34;+
                      &#34;be treated normally, avoiding unneccesary memory use.&#34;)
            
            #give warning for nonexistent dimensions
            if len(dim_range.keys() - set(order)) &gt; 0:
                for dim in dim_range.keys() - set(order):
                    print(&#34;[WARNING] confocal.sp8_series.load_stack: &#34;+
                          &#34;dimension &#39;&#34;+dim+&#34;&#39; not present in data, ignoring &#34;+
                          &#34;this entry.&#34;)
                    dim_range.pop(dim)
            
            #create a tuple of slice objects for each dimension except x and y
            slices = []
            for dim in order[:-2]:
                if not dim in dim_range:
                    dim_range[dim] = slice(None,None)
                slices.append(dim_range[dim])
            slices = tuple(slices)
            
            #reshape the filenames and apply slicing, then ravel back to flat list
            filenames = np.reshape(filenames,newshape[:-2])[slices]
        
        else:
            filenames = np.reshape(filenames,newshape[:-2])
            
        #change dim order if multiple channels, move to 0th axis
        for i,dim in enumerate(order):
            if dim == &#39;channel&#39;:
                filenames = np.moveaxis(filenames,i,0)
                order = [order[i]] + order[:i] + order[i+1:]
        
        #get final shape and flatten list of filenames in correct order
        newshape = list(np.shape(filenames)) + newshape[-2:]
        filenames = list(filenames.ravel())
        
        #load and reshape data
        data = self.load_data(filenames=filenames,dtype=dtype)
        data = np.reshape(data,tuple(newshape))
        
        #if ranges for x or y are chosen, remove those from the array now,
        #account (top to bottom) for trimming x Ã¡nd y, only x, or only y.
        if &#39;x-axis&#39; in dim_range:
            if &#39;y-axis&#39; in dim_range:
                slices = tuple([slice(None)]*len(newshape[:-2]) + [dim_range[&#39;y-axis&#39;],dim_range[&#39;x-axis&#39;]])
            else:
                slices = tuple([slice(None)]*len(newshape[:-1]) + [dim_range[&#39;x-axis&#39;]])
            data = data[slices]
        elif &#39;y-axis&#39; in dim_range:
            slices = tuple([slice(None)]*len(newshape[:-2]) + [dim_range[&#39;y-axis&#39;]])
            data = data[slices]
        
        return data, tuple(order)
    
    def load_metadata(self):
        &#34;&#34;&#34;
        Load the xml metadata exported with the files as xml_root object which
        can be indexed with xml.etree.ElementTree

        Returns
        -------
        metadata : xml.etree.ElementTree object
            Parsable xml tree object containing all the metadata

        &#34;&#34;&#34;
        import xml.etree.ElementTree as et
        
        metadata_path = sorted(glob.glob(os.path.join(os.path.curdir, &#39;MetaData&#39;, &#39;*.xml&#39;)))[0]
        metadata = et.parse(metadata_path)
        metadata = metadata.getroot()
        
        self.metadata = metadata
        return metadata
    
    def get_metadata_channels(self):
        &#34;&#34;&#34;
        Gets the channel information from the metadata

        Returns
        -------
        channels : list of dict
            list of dictionaries with length equal to number of channels where
            each dict contains the metadata for one channel
        &#34;&#34;&#34;
        #Fetch metadata from class instance or load if it was not loaded yet
        try:
            metadata = self.metadata
        except AttributeError:
            metadata = sp8_series.load_metadata(self)
        
        channels = [dict(ch.attrib) for ch in metadata.find(&#39;.//Channels&#39;)]
        
        self.metadata_channels = channels
        return channels
    
    def get_metadata_dimensions(self):
        &#34;&#34;&#34;
        Gets the dimension information from the metadata

        Returns
        -------
        dimensions : list of dict
            list of dictionaries with length number of dimensions where
            each dict contains the metadata for one data dimension

        &#34;&#34;&#34;
        #Fetch metadata from class instance or load if it was not loaded yet
        try:
            metadata = self.metadata
        except AttributeError:
            metadata = sp8_series.load_metadata(self)
        
        dimensions = [dict(dim.attrib) for dim in metadata.find(&#39;.//Dimensions&#39;)]
        
        self.metadata_dimensions = dimensions
        return dimensions
    
    def get_metadata_dimension(self,dim):
        &#34;&#34;&#34;
        Gets the dimension data for a particular dimension. Dimension can be
        given both as integer index (as specified by the Leica exported 
        MetaData which may not correspond to the indexing order of the data
        stack) or as string containing the physical meaning, e.g. &#39;x-axis&#39;,
        &#39;time&#39;, &#39;excitation wavelength&#39;, etc.

        Parameters
        ----------
        dim : int or str
            dimension to get metadata of specified as integer or as name.

        Returns
        -------
        dimension : dict
            dictionary containing all metadata for that dimension

        &#34;&#34;&#34;
        #convert string labels to corresponding integer labels
        if dim == &#39;channel&#39; or dim == 0:
            raise ValueError(&#39;use sp8_series.get_metadata_channels() for &#39;+
                                      &#39;channel data&#39;)
        elif dim == &#39;x-axis&#39;:
            dim = 1
        elif dim == &#39;y-axis&#39;:
            dim = 2
        elif dim == &#39;z-axis&#39;:
            dim = 3
        elif dim == &#39;time&#39;:
            dim = 4
        elif dim == &#39;detection wavelength&#39;:
            dim = 5
        elif dim == &#39;excitation wavelength&#39;:
            dim = 6
        elif type(dim) != int or dim&gt;6 or dim&lt;0:
            raise ValueError(&#39;&#34;&#39;+str(dim)+&#39;&#34; is not a valid dimension label&#39;)
        
        #fetch or load dimensions
        try:
            dimensions = self.metadata_dimensions
        except AttributeError:
            dimensions = self.get_metadata_dimensions()
        
        #find correct dimension in the list of dimensions
        index = [int(d[&#39;DimID&#39;]) for d in dimensions].index(dim)
        dimension = dict(dimensions[index])
        
        return dimension
    
    def get_dimension_steps(self,dim,load_stack_indices=False):
        &#34;&#34;&#34;
        Gets a list of values for each step along the specified dimension, e.g.
        a list of timestamps for the images or a list of height values for all
        slices of a z-stack. For specification of dimensions, see
        sp8_series.get_metadata_dimension()

        Parameters
        ----------
        dim : int or str
            dimension to get steps for
        load_stack_indices : bool
            if True, trims down the list of steps based on the dim_range used
            when last loading data with load_stack

        Returns
        -------
        steps : list
            list of values for every logical step in the data
        unit : str
            physical unit of the step values

        &#34;&#34;&#34;
        #get the data
        dimension = self.get_metadata_dimension(dim)
        
        #obtain the infomation and calculate steps
        start = float(dimension[&#39;Origin&#39;])
        size = float(dimension[&#39;Length&#39;])
        n = int(dimension[&#39;NumberOfElements&#39;])
        unit = dimension[&#39;Unit&#39;]
        steps = np.linspace(start,start+size,n)

        if load_stack_indices:
            try:
                dim_range = self._stack_dim_range
            except AttributeError:
                raise AttributeError(&#39;data must be loaded with &#39;+
                                  &#39;sp8_series.load_stack() prior to &#39;+
                                  &#39;calling visitech_faststack.get_timestamps()&#39;
                                  +&#39; with load_stack_indices=True&#39;)

            if dim in dim_range:
                steps = steps[dim_range[dim]]

        return steps,unit
    
    def get_dimension_stepsize(self,dim):
        &#34;&#34;&#34;
        Get the size of a single step along the specified dimension, e.g.
        the pixelsize in x, y or z, or the time between timesteps. For
        specification of dimensions, see sp8_series.get_metadata_dimension()

        Parameters
        ----------
        dim : int or str
            dimension to get stepsize for

        Returns
        -------
        value : float
            stepsize
        unit : int
            physical unit of value

        &#34;&#34;&#34;
        #get the data
        dimension = self.get_metadata_dimension(dim)
        
        #obtain the infomation and calculate steps
        size = float(dimension[&#39;Length&#39;])
        n = int(dimension[&#39;NumberOfElements&#39;])
        unit = dimension[&#39;Unit&#39;]
        
        return size/(n-1),unit
    
    def get_series_name(self):
        &#34;&#34;&#34;
        Returns a string containing the filename (sans file extension) under 
        which the series is saved.

        Returns
        -------
        name : str
            name of the series

        &#34;&#34;&#34;

        #find metadata file in subfolder, split off location and extension
        path = os.path.join(os.path.curdir, &#39;MetaData&#39;, &#39;*.xml&#39;)
        path = sorted(glob.glob(path))[0]
        return os.path.split(path)[1][:-4]</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="scm_confocal.sp8_series.get_dimension_steps"><code class="name flex">
<span>def <span class="ident">get_dimension_steps</span></span>(<span>self, dim, load_stack_indices=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets a list of values for each step along the specified dimension, e.g.
a list of timestamps for the images or a list of height values for all
slices of a z-stack. For specification of dimensions, see
sp8_series.get_metadata_dimension()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>dimension to get steps for</dd>
<dt><strong><code>load_stack_indices</code></strong> :&ensp;<code>bool</code></dt>
<dd>if True, trims down the list of steps based on the dim_range used
when last loading data with load_stack</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>steps</code></strong> :&ensp;<code>list</code></dt>
<dd>list of values for every logical step in the data</dd>
<dt><strong><code>unit</code></strong> :&ensp;<code>str</code></dt>
<dd>physical unit of the step values</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dimension_steps(self,dim,load_stack_indices=False):
    &#34;&#34;&#34;
    Gets a list of values for each step along the specified dimension, e.g.
    a list of timestamps for the images or a list of height values for all
    slices of a z-stack. For specification of dimensions, see
    sp8_series.get_metadata_dimension()

    Parameters
    ----------
    dim : int or str
        dimension to get steps for
    load_stack_indices : bool
        if True, trims down the list of steps based on the dim_range used
        when last loading data with load_stack

    Returns
    -------
    steps : list
        list of values for every logical step in the data
    unit : str
        physical unit of the step values

    &#34;&#34;&#34;
    #get the data
    dimension = self.get_metadata_dimension(dim)
    
    #obtain the infomation and calculate steps
    start = float(dimension[&#39;Origin&#39;])
    size = float(dimension[&#39;Length&#39;])
    n = int(dimension[&#39;NumberOfElements&#39;])
    unit = dimension[&#39;Unit&#39;]
    steps = np.linspace(start,start+size,n)

    if load_stack_indices:
        try:
            dim_range = self._stack_dim_range
        except AttributeError:
            raise AttributeError(&#39;data must be loaded with &#39;+
                              &#39;sp8_series.load_stack() prior to &#39;+
                              &#39;calling visitech_faststack.get_timestamps()&#39;
                              +&#39; with load_stack_indices=True&#39;)

        if dim in dim_range:
            steps = steps[dim_range[dim]]

    return steps,unit</code></pre>
</details>
</dd>
<dt id="scm_confocal.sp8_series.get_dimension_stepsize"><code class="name flex">
<span>def <span class="ident">get_dimension_stepsize</span></span>(<span>self, dim)</span>
</code></dt>
<dd>
<section class="desc"><p>Get the size of a single step along the specified dimension, e.g.
the pixelsize in x, y or z, or the time between timesteps. For
specification of dimensions, see sp8_series.get_metadata_dimension()</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>dimension to get stepsize for</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>value</code></strong> :&ensp;<code>float</code></dt>
<dd>stepsize</dd>
<dt><strong><code>unit</code></strong> :&ensp;<code>int</code></dt>
<dd>physical unit of value</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dimension_stepsize(self,dim):
    &#34;&#34;&#34;
    Get the size of a single step along the specified dimension, e.g.
    the pixelsize in x, y or z, or the time between timesteps. For
    specification of dimensions, see sp8_series.get_metadata_dimension()

    Parameters
    ----------
    dim : int or str
        dimension to get stepsize for

    Returns
    -------
    value : float
        stepsize
    unit : int
        physical unit of value

    &#34;&#34;&#34;
    #get the data
    dimension = self.get_metadata_dimension(dim)
    
    #obtain the infomation and calculate steps
    size = float(dimension[&#39;Length&#39;])
    n = int(dimension[&#39;NumberOfElements&#39;])
    unit = dimension[&#39;Unit&#39;]
    
    return size/(n-1),unit</code></pre>
</details>
</dd>
<dt id="scm_confocal.sp8_series.get_metadata_channels"><code class="name flex">
<span>def <span class="ident">get_metadata_channels</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets the channel information from the metadata</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>channels</code></strong> :&ensp;<code>list</code> of <code>dict</code></dt>
<dd>list of dictionaries with length equal to number of channels where
each dict contains the metadata for one channel</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata_channels(self):
    &#34;&#34;&#34;
    Gets the channel information from the metadata

    Returns
    -------
    channels : list of dict
        list of dictionaries with length equal to number of channels where
        each dict contains the metadata for one channel
    &#34;&#34;&#34;
    #Fetch metadata from class instance or load if it was not loaded yet
    try:
        metadata = self.metadata
    except AttributeError:
        metadata = sp8_series.load_metadata(self)
    
    channels = [dict(ch.attrib) for ch in metadata.find(&#39;.//Channels&#39;)]
    
    self.metadata_channels = channels
    return channels</code></pre>
</details>
</dd>
<dt id="scm_confocal.sp8_series.get_metadata_dimension"><code class="name flex">
<span>def <span class="ident">get_metadata_dimension</span></span>(<span>self, dim)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets the dimension data for a particular dimension. Dimension can be
given both as integer index (as specified by the Leica exported
MetaData which may not correspond to the indexing order of the data
stack) or as string containing the physical meaning, e.g. 'x-axis',
'time', 'excitation wavelength', etc.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dim</code></strong> :&ensp;<code>int</code> or <code>str</code></dt>
<dd>dimension to get metadata of specified as integer or as name.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dimension</code></strong> :&ensp;<code>dict</code></dt>
<dd>dictionary containing all metadata for that dimension</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata_dimension(self,dim):
    &#34;&#34;&#34;
    Gets the dimension data for a particular dimension. Dimension can be
    given both as integer index (as specified by the Leica exported 
    MetaData which may not correspond to the indexing order of the data
    stack) or as string containing the physical meaning, e.g. &#39;x-axis&#39;,
    &#39;time&#39;, &#39;excitation wavelength&#39;, etc.

    Parameters
    ----------
    dim : int or str
        dimension to get metadata of specified as integer or as name.

    Returns
    -------
    dimension : dict
        dictionary containing all metadata for that dimension

    &#34;&#34;&#34;
    #convert string labels to corresponding integer labels
    if dim == &#39;channel&#39; or dim == 0:
        raise ValueError(&#39;use sp8_series.get_metadata_channels() for &#39;+
                                  &#39;channel data&#39;)
    elif dim == &#39;x-axis&#39;:
        dim = 1
    elif dim == &#39;y-axis&#39;:
        dim = 2
    elif dim == &#39;z-axis&#39;:
        dim = 3
    elif dim == &#39;time&#39;:
        dim = 4
    elif dim == &#39;detection wavelength&#39;:
        dim = 5
    elif dim == &#39;excitation wavelength&#39;:
        dim = 6
    elif type(dim) != int or dim&gt;6 or dim&lt;0:
        raise ValueError(&#39;&#34;&#39;+str(dim)+&#39;&#34; is not a valid dimension label&#39;)
    
    #fetch or load dimensions
    try:
        dimensions = self.metadata_dimensions
    except AttributeError:
        dimensions = self.get_metadata_dimensions()
    
    #find correct dimension in the list of dimensions
    index = [int(d[&#39;DimID&#39;]) for d in dimensions].index(dim)
    dimension = dict(dimensions[index])
    
    return dimension</code></pre>
</details>
</dd>
<dt id="scm_confocal.sp8_series.get_metadata_dimensions"><code class="name flex">
<span>def <span class="ident">get_metadata_dimensions</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Gets the dimension information from the metadata</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dimensions</code></strong> :&ensp;<code>list</code> of <code>dict</code></dt>
<dd>list of dictionaries with length number of dimensions where
each dict contains the metadata for one data dimension</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata_dimensions(self):
    &#34;&#34;&#34;
    Gets the dimension information from the metadata

    Returns
    -------
    dimensions : list of dict
        list of dictionaries with length number of dimensions where
        each dict contains the metadata for one data dimension

    &#34;&#34;&#34;
    #Fetch metadata from class instance or load if it was not loaded yet
    try:
        metadata = self.metadata
    except AttributeError:
        metadata = sp8_series.load_metadata(self)
    
    dimensions = [dict(dim.attrib) for dim in metadata.find(&#39;.//Dimensions&#39;)]
    
    self.metadata_dimensions = dimensions
    return dimensions</code></pre>
</details>
</dd>
<dt id="scm_confocal.sp8_series.get_series_name"><code class="name flex">
<span>def <span class="ident">get_series_name</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns a string containing the filename (sans file extension) under
which the series is saved.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>name of the series</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_series_name(self):
    &#34;&#34;&#34;
    Returns a string containing the filename (sans file extension) under 
    which the series is saved.

    Returns
    -------
    name : str
        name of the series

    &#34;&#34;&#34;

    #find metadata file in subfolder, split off location and extension
    path = os.path.join(os.path.curdir, &#39;MetaData&#39;, &#39;*.xml&#39;)
    path = sorted(glob.glob(path))[0]
    return os.path.split(path)[1][:-4]</code></pre>
</details>
</dd>
<dt id="scm_confocal.sp8_series.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self, filenames=None, first=None, last=None, dtype=numpy.uint8)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads the sequence of images into ndarray of form (files,y,x) and
converts the data to dtype</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filenames</code></strong> :&ensp;<code>list</code> of <code>str</code>, optional</dt>
<dd>filenames of images to load. The default is what is passed from
<strong>init</strong>, which by default is all .tif images in the current
working directory.</dd>
<dt><strong><code>first</code></strong> :&ensp;<code>None</code> or <code>int</code>, optional</dt>
<dd>index of first image to load. The default is None.</dd>
<dt><strong><code>last</code></strong> :&ensp;<code>None</code> or <code>int</code>, optional</dt>
<dd>index of last image to load plus one. The default is None.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;(<code>numpy</code>) <code>datatype</code>, optional</dt>
<dd>type to scale data to. The default is np.uint8.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>3d numpy array with dimension order (filenames,y,x).</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(self, filenames=None, first=None, last=None, dtype=np.uint8):
    &#34;&#34;&#34;
    Loads the sequence of images into ndarray of form (files,y,x) and
    converts the data to dtype

    Parameters
    ----------
    filenames : list of str, optional
        filenames of images to load. The default is what is passed from 
        __init__, which by default is all .tif images in the current
        working directory.
    first : None or int, optional
        index of first image to load. The default is None.
    last : None or int, optional
        index of last image to load plus one. The default is None.
    dtype : (numpy) datatype, optional
        type to scale data to. The default is np.uint8.

    Returns
    -------
    data : numpy.ndarray
        3d numpy array with dimension order (filenames,y,x).

    &#34;&#34;&#34;
    import PIL.Image
    
    if filenames == None:
        filenames = sorted(self.filenames)
    
    data = np.array([np.array(PIL.Image.open(name)) for name in filenames[first:last]])

    #check if images are 2D (i.e. greyscale)
    if data.ndim &gt; 3:
        print(&#34;[WARNING] sp8_series.load_data(): images do not have the correct dimensionality, &#34;+
              &#34;did you load colour images perhaps? Continueing with average values of higher dimensions&#34;)
        data = np.mean(data,axis=tuple(range(3,data.ndim)),dtype=dtype)


    if not data.dtype == dtype:
        data = data/np.amax(data)*np.iinfo(dtype).max
        data = data.astype(dtype)
    self.data = data
    return data</code></pre>
</details>
</dd>
<dt id="scm_confocal.sp8_series.load_metadata"><code class="name flex">
<span>def <span class="ident">load_metadata</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Load the xml metadata exported with the files as xml_root object which
can be indexed with xml.etree.ElementTree</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>metadata</code></strong> :&ensp;<code>xml.etree.ElementTree</code> <code>object</code></dt>
<dd>Parsable xml tree object containing all the metadata</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_metadata(self):
    &#34;&#34;&#34;
    Load the xml metadata exported with the files as xml_root object which
    can be indexed with xml.etree.ElementTree

    Returns
    -------
    metadata : xml.etree.ElementTree object
        Parsable xml tree object containing all the metadata

    &#34;&#34;&#34;
    import xml.etree.ElementTree as et
    
    metadata_path = sorted(glob.glob(os.path.join(os.path.curdir, &#39;MetaData&#39;, &#39;*.xml&#39;)))[0]
    metadata = et.parse(metadata_path)
    metadata = metadata.getroot()
    
    self.metadata = metadata
    return metadata</code></pre>
</details>
</dd>
<dt id="scm_confocal.sp8_series.load_stack"><code class="name flex">
<span>def <span class="ident">load_stack</span></span>(<span>self, dim_range={}, dtype=numpy.uint8)</span>
</code></dt>
<dd>
<section class="desc"><p>Similar to sp8_series.load_data(), but converts the 3D array of images
automatically to a np.ndarray of the appropriate dimensionality.</p>
<p>Array dimensions are specified as follows:
- If the number of detector channels is 2 or higher, the first
array axis is the detector channel index (named 'channel').
- If the number of channels is 1, the first array axis is the first
available dimension (instead of 'channel').
- Each subsequent array axis corresponds to a dimension as
specified by and in reversed order of the metadata exported by
the microscope software, excluding dimensions which are not
available. The default order of dimensions in the metadata is:
- (0 = 'channel')
-
1 = 'x-axis'
-
2 = 'y-axis'
-
3 = 'z-axis'
-
4 = 'time'
-
5 = 'detection wavelength'
-
6 = 'excitation wavelength'</p>
<pre><code>- As an example, a 2 channel xyt measurement would result in a 4-d
  array with axis order ('channel','time','y-axis',
  'x-axis'), and a single channel xyz scan would be returned as
  ('z-axis','y-axis','x-axis')
</code></pre>
<p>For loading only part of the total dataset, the dim_range parameter can
be used to specify a range along any of the dimensions. This will be
more memory efficient than loading the entire stack and then discarding
part of the data. For slicing along the x or y axis this is not
possible and whole (xy) images must be loaded prior to discarding
data outside the specified x or y axis range.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dim_range</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>dict, with keys corresponding to channel/dimension labels as above
and slice objects as values. This allows you to only load part of
the data along any of the dimensions, such as only loading one
channel of multichannel data or a particular z-range. An example
use for only taking time steps up to 5 and z-slice 20 to 30 would
be:
dim_range={'time':slice(None,5), 'z-axis':slice(20,30)}.
The default is {}.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;(<code>numpy</code>) <code>datatype</code>, optional</dt>
<dd>type to scale data to. The default is np.uint8.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>ndarray with the pixel values</dd>
<dt><strong><code>dimorder</code></strong> :&ensp;<code>tuple</code></dt>
<dd>tuple with lenght data.ndim specifying the ordering of dimensions
in the data with labels from the metadata of the microscope.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_stack(self,dim_range={},dtype=np.uint8):
    &#34;&#34;&#34;
    Similar to sp8_series.load_data(), but converts the 3D array of images
    automatically to a np.ndarray of the appropriate dimensionality.
    
    Array dimensions are specified as follows:
        - If the number of detector channels is 2 or higher, the first
          array axis is the detector channel index (named &#39;channel&#39;).
        - If the number of channels is 1, the first array axis is the first
          available dimension (instead of &#39;channel&#39;).
        - Each subsequent array axis corresponds to a dimension as
          specified by and in reversed order of the metadata exported by
          the microscope software, excluding dimensions which are not
          available. The default order of dimensions in the metadata is:
             - (0 = &#39;channel&#39;)
             -  1 = &#39;x-axis&#39;
             -  2 = &#39;y-axis&#39;
             -  3 = &#39;z-axis&#39;
             -  4 = &#39;time&#39;
             -  5 = &#39;detection wavelength&#39;
             -  6 = &#39;excitation wavelength&#39;
    
        - As an example, a 2 channel xyt measurement would result in a 4-d
          array with axis order (&#39;channel&#39;,&#39;time&#39;,&#39;y-axis&#39;,
          &#39;x-axis&#39;), and a single channel xyz scan would be returned as
          (&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;)
    
    For loading only part of the total dataset, the dim_range parameter can
    be used to specify a range along any of the dimensions. This will be
    more memory efficient than loading the entire stack and then discarding
    part of the data. For slicing along the x or y axis this is not
    possible and whole (xy) images must be loaded prior to discarding
    data outside the specified x or y axis range.

    Parameters
    ----------
    dim_range : dict, optional
        dict, with keys corresponding to channel/dimension labels as above
        and slice objects as values. This allows you to only load part of
        the data along any of the dimensions, such as only loading one
        channel of multichannel data or a particular z-range. An example
        use for only taking time steps up to 5 and z-slice 20 to 30 would
        be:
            dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
        The default is {}.
    dtype : (numpy) datatype, optional
        type to scale data to. The default is np.uint8.

    Returns
    -------
    data : numpy.ndarray
        ndarray with the pixel values
    dimorder : tuple
        tuple with lenght data.ndim specifying the ordering of dimensions
        in the data with labels from the metadata of the microscope.
    &#34;&#34;&#34;

    #load the metadata
    try:
        channels = self.metadata_channels
    except AttributeError:
        channels = sp8_series.get_metadata_channels(self)
    try:
        dimensions = self.metadata_dimensions
    except AttributeError:
        dimensions = sp8_series.get_metadata_dimensions(self)
    
    #determine what the new shape should be from dimensional metadata
    newshape = [int(dim[&#39;NumberOfElements&#39;]) for dim in reversed(dimensions)]
    
    #replace dimID with more sensible label
    def DimIDreplace(idlist):
        pattern = zip(list(&#39;123456&#39;),[&#39;x-axis&#39;,&#39;y-axis&#39;,&#39;z-axis&#39;,&#39;time&#39;,
                      &#39;detection wavelength&#39;,&#39;emission wavelength&#39;])
        for l,r in pattern:
            idlist = idlist.replace(l,r)
        return idlist
    
    order = [DimIDreplace(dim[&#39;DimID&#39;]) for dim in reversed(dimensions)]
    
    #append channel (but before x and y) information for multichannel data
    if len(channels)&gt;1:
        newshape = newshape[:-2] + [len(channels)] + newshape[-2:]
        order = order[:-2] + [&#39;channel&#39;] + order[-2:]
    
    #load filenames
    filenames = self.filenames

    #apply slicing to the list of filenames before loading images
    if len(dim_range) &gt; 0:

        self._stack_dim_range = dim_range

        #give a warning that only whole xy images are loaded
        if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
            print(&#34;[WARNING] confocal.sp8_series.load_stack: Loading only&#34;+
                  &#34; part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
                  &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
                  &#34;into memory before discarding values outside of the &#34;+
                  &#34;slice range specified for the x-axis and/or y-axis. &#34;+
                  &#34;Other axes for which a range is specified will still &#34;+
                  &#34;be treated normally, avoiding unneccesary memory use.&#34;)
        
        #give warning for nonexistent dimensions
        if len(dim_range.keys() - set(order)) &gt; 0:
            for dim in dim_range.keys() - set(order):
                print(&#34;[WARNING] confocal.sp8_series.load_stack: &#34;+
                      &#34;dimension &#39;&#34;+dim+&#34;&#39; not present in data, ignoring &#34;+
                      &#34;this entry.&#34;)
                dim_range.pop(dim)
        
        #create a tuple of slice objects for each dimension except x and y
        slices = []
        for dim in order[:-2]:
            if not dim in dim_range:
                dim_range[dim] = slice(None,None)
            slices.append(dim_range[dim])
        slices = tuple(slices)
        
        #reshape the filenames and apply slicing, then ravel back to flat list
        filenames = np.reshape(filenames,newshape[:-2])[slices]
    
    else:
        filenames = np.reshape(filenames,newshape[:-2])
        
    #change dim order if multiple channels, move to 0th axis
    for i,dim in enumerate(order):
        if dim == &#39;channel&#39;:
            filenames = np.moveaxis(filenames,i,0)
            order = [order[i]] + order[:i] + order[i+1:]
    
    #get final shape and flatten list of filenames in correct order
    newshape = list(np.shape(filenames)) + newshape[-2:]
    filenames = list(filenames.ravel())
    
    #load and reshape data
    data = self.load_data(filenames=filenames,dtype=dtype)
    data = np.reshape(data,tuple(newshape))
    
    #if ranges for x or y are chosen, remove those from the array now,
    #account (top to bottom) for trimming x Ã¡nd y, only x, or only y.
    if &#39;x-axis&#39; in dim_range:
        if &#39;y-axis&#39; in dim_range:
            slices = tuple([slice(None)]*len(newshape[:-2]) + [dim_range[&#39;y-axis&#39;],dim_range[&#39;x-axis&#39;]])
        else:
            slices = tuple([slice(None)]*len(newshape[:-1]) + [dim_range[&#39;x-axis&#39;]])
        data = data[slices]
    elif &#39;y-axis&#39; in dim_range:
        slices = tuple([slice(None)]*len(newshape[:-2]) + [dim_range[&#39;y-axis&#39;]])
        data = data[slices]
    
    return data, tuple(order)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scm_confocal.util"><code class="flex name class">
<span>class <span class="ident">util</span></span>
<span>(</span><span>*args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Set of utility functions for dealing with stacks and confocal data</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class util:
    &#34;&#34;&#34;
    Set of utility functions for dealing with stacks and confocal data
    &#34;&#34;&#34;
    
    def bin_stack(images,n=1,blocksize=None,quiet=False,dtype=np.uint8):
        &#34;&#34;&#34;
        bins numpy ndarrays in arbitrary dimensions by a factor n. Prior to
        binning, elements from the end are deleted until the length is a
        multiple of the bin factor. Executes averaging of bins in floating
        point precision, which is memory intensive for large stacks. Using
        smaller blocks reduces memory usage, but is less efficient.
        Parameters
        ----------
        images : numpy.ndarray
            ndarray containing the data
        n : int or tuple of int, optional
            factor to bin with for all dimensions (int) or each dimension
            individually (tuple with one int per dimension). The default is 1.
        blocksize : int, optional
            number of (binned) slices to process at a time to conserve memory.
            The default is entire stack.
        quiet : bool, optional
            suppresses printed output when True. The default is False.
        dtype : (numpy) datatype, optional
            datatype to use for output. Averaging of the binned pixels always
            occurs in floating point precision. The default is np.uint8.
        Returns
        -------
        images : numpy.ndarray
            binned stack
        &#34;&#34;&#34;
        dims = images.ndim
        
        #check input parameters for type
        if type(n) != tuple and type(n) != int:
            print(&#39;n must be int or tuple of ints, skipping binning step&#39;)
            return images
        
        #when n is int, convert to tuple of ints with length dims
        if type(n) == int:
            n = (n,)*dims
            
        #else if n is a tuple, check if length of n matches dims
        elif len(n) != dims:
            print(&#39;number of dimensions does not match, skipping binning step&#39;)
            return images
        
        #skip rest of code when every element in n is 1
        if all([nitem==1 for nitem in n]):
            if not quiet:
                print(&#39;no binning used&#39;)
            return images
    
        #define new shapes
        oldshape = np.shape(images)
        trimmedshape = tuple([int(oldshape[d] - oldshape[d] % n[d]) for d in range(dims)])
        newshape = tuple([int(trimmedshape[d]/n[d]) for d in range(dims)])
        
        #trim ends when new shape is not a whole multiple of binfactor
        slices = [range(trimmedshape[d],oldshape[d]) for d in range(dims)]
        for d in range(dims):
            if trimmedshape[d] != oldshape[d]:
                images = np.delete(images,slices[d],axis=d)
        
        #print old and new shapes when trimming and or binning is used
        if oldshape != trimmedshape and not quiet:
            print(&#39;trimming from shape {} to {}, binning to {}...&#39;.format(oldshape,trimmedshape,newshape))
        elif not quiet:
            print(&#39;binning from shape {} to {}...&#39;.format(trimmedshape,newshape))
        
        #check block size
        if blocksize == None:
            blocksize = newshape[0]
        
        #determine shape such that reshaped array has dims*2 dimensions, with each dim spread over new axis to bin along
        reshapeshape = [i for subtuple in [(newshape[d],n[d]) for d in range(dims)] for i in subtuple]
        
        #when binning entire array at once
        if blocksize &gt;= newshape[0]:
            #execute reshape
            images = images.reshape(reshapeshape)
            
            #average along binning axes to obtain original dimensionality
            #(floating point precision and memory intensive!!)
            for d in reversed(range(1,2*dims,2)):
                images = images.mean(d)
            
            #set type back from floating point
            images = images.astype(dtype)
            
        #when splitting binning into multiple blocks to conserve memory
        else:
            if not quiet:
                print(&#39;splitting binning in {} blocks&#39;.format(int(np.ceil(newshape[0]/blocksize))))
            
            #reshape stack and store to temp variable
            reshapeim = images.reshape(reshapeshape)
            
            #overwrite images with empty array of new shape
            images = np.zeros(newshape,dtype=dtype)
            
            #perform binning in steps of blocksize
            for i in range(0,newshape[0] - (newshape[0] % blocksize),blocksize):
                block = reshapeim[i:i+blocksize]
                for d in reversed(range(1,2*dims,2)):
                    block = np.mean(block,axis=d)
                images[i:i+blocksize] = block.astype(dtype)
            
            #execute last (smaller) block if newshape is not devisible by blocksize
            if newshape[0] % blocksize != 0:
                block = reshapeim[-(newshape[0] % blocksize):]
                for d in reversed(range(1,2*dims,2)):
                    block = np.mean(block,axis=d)
                images[-(newshape[0] % blocksize):] = block.astype(dtype)
        
        if not quiet:
            print(&#39;binning finished&#39;)
        
        return images
    
    def fit_powerlaw(x,y,weights=None):
        &#34;&#34;&#34;
        Linear regression in log space of the MSD to get diffusion constant, which
        is a powerlaw in linear space of the form A*x**n
        Parameters
        ----------
        x : list or numpy.array
            x coordinates of data points to fit
        y : list or numpy.array
            y coordinates of data points to fit
        weights : list or numpy.array, optional
            list of weights to use for each (x,y) coordinate. The default is 
            None.
        Returns
        -------
        A : float
            constant A
        n : float
            power n
        sigmaA : float
            standard deviation in A
        sigmaN : float
            standard deviation in n
        &#34;&#34;&#34;
        import scipy.optimize
        
        def f(x,a,b):
            return a*x + b
        
        if weights is None:
            weights = np.ones(len(y))
        else:
            weights = np.array(weights)
        
        #remove nan values
        x = np.array(x)
        y = np.array(y)
        
        x = x[~np.isnan(y)]
        weights = weights[~np.isnan(y)]
        y = y[~np.isnan(y)]
        
        #fit
        (n,A), covariance = scipy.optimize.curve_fit(f,np.log(x),np.log(y),sigma=weights)
        sigmaN,sigmaA = np.sqrt(np.diag(covariance))
        A = np.exp(A)
        sigmaA = sigmaA*np.exp(A)
        
        return A,n,sigmaA,sigmaN
    
    def mean_square_displacement(features, pos_cols = [&#39;x&#39;,&#39;y&#39;,&#39;z&#39;], t_col=&#39;t (s)&#39;,
                             nparticles=None, pickrandom=False, nbins=20,
                             tmin=None, tmax=None):
        &#34;&#34;&#34;
        calculate the mean square displacement vs time for linked particles
        Parameters
        ----------
        features : pandas.DataFrame
            output from trackpy.link containing tracking data
        pos_cols : list of str, optional
            names of columns to use for coordinates. The default is
            [&#39;x&#39;,&#39;y&#39;,&#39;z&#39;].
        t_col : str, optional
            name of column containing timestamps. The default is &#39;t (s)&#39;.
        nparticles : int, optional
            number of particles to use for calculation (useful for large
            datasets). The default is all particles.
        pickrandom : bool, optional
            whether to pick nparticles randomly or not, if False it takes the 
            n longest tracked particles from data. The default is False.
        nbins : int, optional
            number of bins for output. The default is 20.
        tmin : float, optional
            left edge of first bin. The default is min(t_col).
        tmax : float, optional
           right edge of last bin, The default is max(t_col).
        Returns
        -------
        binedges : numpy.array
            edges of time bins
        bincounts : numpy.array
            number of sampling points for each bin
        binmeans : numpy.array
            mean square displacement values
        &#34;&#34;&#34;
        #restructure data to correct order
        features = features[[&#39;particle&#39;]+[t_col]+pos_cols]
        features = features.set_index(&#39;particle&#39;)
        dims = len(pos_cols)
        
        #initialize empty array to contain [[dt1,dr1],[dt2,dr2],...]
        dt_dr = np.empty((0,2))
        
        #converting to set assures unique values only
        particles = set(features.index)
        
        #optionally take a subset of particles
        if nparticles != None and len(particles)&gt;nparticles:
            
            #optionally take random subset of particles
            if pickrandom:
                import random
                particles = random.sample(set(features.index),nparticles)
            
            #else take the particles which occur in most of the frames
            else:
                vals, counts = np.unique(features.index, return_counts=True)
                sortedindices = np.argsort(-counts)[:nparticles]
                particles = vals[sortedindices]
        
        #iterate over all particles and all time intervals for that particle and 
        # append [[dr,dt]] to dt_dr each time
        for p in particles:
            pdata = features.loc[p]
            for j in range(len(pdata)):
                for i in range(j):
                    dt_dr = np.append(
                            dt_dr,
                            [[
                                    pdata.iat[j,0] - pdata.iat[i,0],
                                    sum([(pdata.iat[j,d] - pdata.iat[i,d])**2 for d in range(1,dims+1)])
                            ]],
                            axis = 0
                            )
        
        #check bins
        if tmin == None:
            tmin = min(dt_dr[:,0])
        if tmax == None:
            tmax = max(dt_dr[:,0])
        
        #create bin edges
        binedges = np.linspace(tmin,tmax,nbins+1,endpoint=True)
        
        #put each timestep into the correct bin and remove data left of first bin
        binpos = binedges.searchsorted(dt_dr[:,0],side=&#39;right&#39;)
        dt_dr = dt_dr[binpos!=0]
        binpos = binpos[binpos!=0]
        binpos = binpos-1
        
        #count each bin and weigh counts by corresponding r, then normalize by unweighted counts
        bincounts = np.bincount(binpos, minlength=nbins)
        binmeans =  np.bincount(binpos, weights=dt_dr[:,1], minlength=nbins) / bincounts
        
        return binedges,bincounts[:-1],binmeans[:-1]
    
    def mean_square_displacement_per_frame(features, pos_cols = [&#39;x&#39;,&#39;y&#39;], feat_col = &#39;particle&#39;):
        &#34;&#34;&#34;
        Calculate the mean square movement of all tracked features between
        subsequent frames using efficient pandas linear algebra
        Parameters
        ----------
        features : pandas.Dataframe
            dataframe containing the tracking data over timesteps indexed by
            frame number and containing coordinates of features.
        pos_cols : list of str, optional
            names of the columns containing coordinates. The default is
            [&#39;x&#39;,&#39;y&#39;].
        feat_col : str
            name of column containing feature identifyers. The default is
            &#39;particle&#39;.
        Returns
        -------
        msd : numpy.array
            averages of the squared displacements between each two steps
        &#34;&#34;&#34;

        nf = int(max(features.index))
        
        features = features[[feat_col]+pos_cols]
        msd = np.empty((nf))
        
        #loop over all sets of subsequent frames 
        for i in range(nf):
            
            #create subsets for current and next frame
            a = features.loc[i].set_index(feat_col)
            b = features.loc[i+1].set_index(feat_col)
            
            #find all features which occur in both frames
            f = set(a.index).intersection(set(b.index))
            
            #join positional columns of particles to a single DataFrame
            a = a.loc[f][pos_cols].join(b.loc[f][pos_cols], rsuffix=&#39;_b&#39;)
            
            #create a new column with sum of squared displacement along each direction
            a[&#39;dr**2&#39;] = np.sum([(a[pos + &#39;_b&#39;] - a[pos])**2 for pos in pos_cols], 0)
            
            #take the mean of all square displacement column and add to list
            msd[i] = a[&#39;dr**2&#39;].mean()
        
        return msd

    def subtract_background(images, val=0, percentile=False):
        &#34;&#34;&#34;
        subtract a constant value from a numpy array without going below 0
        Parameters
        ----------
        images : numpy ndarray
            images to correct.
        percentile : bool, optional
            Whether to give the value as a percentile of the stack rather than
            an absolute value to subtrackt. The default is False.
        val : int or float, optional
            Value or percentile to subtract. The default is 0.
        Returns
        -------
        images : numpy ndarray
            the corrected stack.
        &#34;&#34;&#34;

        #calculate percentile val
        if percentile:
            val = np.percentile(images,val)

        #correct intensity
        images[images&lt;val] = 0
        images[images&gt;=val] = images[images&gt;=val]-val

        return images

    def plot_stack_histogram(images,bin_edges=range(0,256),newfig=True,legendname=None,title=&#39;intensity histogram&#39;):
        &#34;&#34;&#34;
        manually flattens list of images to list of pixel values and plots
        histogram. Can combine multiple calls with newfig and legendname
        options
        Parameters
        ----------
        images : numpy ndarray
            array containing pixel values
        bin_edges : list or range, optional
            edges of bins to use. The default is range(0,256).
        newfig : bool, optional
            Whether to open a new figure or to add to currently active figure.
            The default is True.
        legendname : string, optional
            label to use for the legend. The default is None.
        title : string, optional
            text to use as plot title. The default is &#39;intensity histogram&#39;.
        Returns
        -------
        pyplot figure handle
        &#34;&#34;&#34;
        from matplotlib import pyplot as plt
        
        if newfig:
            fig = plt.figure()
            plt.xlabel(&#39;grey value&#39;)
            plt.ylabel(&#39;counts&#39;)
            plt.title(title)
        else:
            fig = plt.gcf()
        
        plt.hist(np.ravel(images),log=(False,True),bins=bin_edges,label=legendname)
        
        if not legendname == None:
            plt.legend()
        plt.show()
        return fig
    
    def multiply_intensity(data,factor,dtype=None):
        &#34;&#34;&#34;
        For multiplying the values of a numpy array while accounting for
        integer overflow issues in integer datatypes. Corrected values larger
        than the datatype max are set to the max value.
        Parameters
        ----------
        data : numpy.ndarray
            array containing the data values
        factor : float
            factor to multiply data with
        dtype : (numpy) datatype, optional
            Datatype to scale data to. The default is the same type as the
            input data.
        Returns
        -------
        data : numpy.ndarray
            data with new intensity values.
        &#34;&#34;&#34;

        if factor == 1:
            return data
        
        if dtype == None:
            dtype = data.dtype
        
        #try if integer type, if not it must be float so one can just multiply
        try:
            maxval = np.iinfo(dtype).max
        except ValueError:
            return data*factor
        
        #for integer types, account for integer overflow
        data[data &gt;= maxval/factor] = maxval
        data[data &lt;  maxval/factor] = data[data &lt;  maxval/factor] * factor
        
        return data
    
    def saveprompt(question=&#34;Save/overwrite? 1=YES, 0=NO. &#34;):
        &#34;&#34;&#34;
        aks user to save, returns boolean
        &#34;&#34;&#34;
        try:
            savefile = int(input(question))
        except ValueError:
            savefile = 0
        if savefile&gt;1 or savefile&lt;0:
            savefile = 0
        if savefile==1:
             print(&#34;saving data&#34;)
             save = True
        else:
            print(&#34;not saving input parameters&#34;)
            save = False
        return save
    
    def write_textfile(params,filename=&#34;parameters.txt&#34;):
        &#34;&#34;&#34;
        stores parameter names and values in text file
        Parameters
        ----------
        params : dictionary of name:value
            the data to store
        filename : str, optional
            file name to us for saving. The default is &#34;parameters.txt&#34;.
        Returns
        -------
        None.
        &#34;&#34;&#34;

        with open(filename,&#39;w&#39;) as file:
            for key,val in params.items():
                file.write(str(key)+&#39; = &#39;+str(val)+&#39;\n&#39;)
        print(&#34;input parameters saved in&#34;,filename)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="scm_confocal.util.bin_stack"><code class="name flex">
<span>def <span class="ident">bin_stack</span></span>(<span>images, n=1, blocksize=None, quiet=False, dtype=numpy.uint8)</span>
</code></dt>
<dd>
<section class="desc"><p>bins numpy ndarrays in arbitrary dimensions by a factor n. Prior to
binning, elements from the end are deleted until the length is a
multiple of the bin factor. Executes averaging of bins in floating
point precision, which is memory intensive for large stacks. Using
smaller blocks reduces memory usage, but is less efficient.
Parameters</p>
<hr>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>ndarray containing the data</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>int</code> or <code>tuple</code> of <code>int</code>, optional</dt>
<dd>factor to bin with for all dimensions (int) or each dimension
individually (tuple with one int per dimension). The default is 1.</dd>
<dt><strong><code>blocksize</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of (binned) slices to process at a time to conserve memory.
The default is entire stack.</dd>
<dt><strong><code>quiet</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>suppresses printed output when True. The default is False.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;(<code>numpy</code>) <code>datatype</code>, optional</dt>
<dd>datatype to use for output. Averaging of the binned pixels always
occurs in floating point precision. The default is np.uint8.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>binned stack</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def bin_stack(images,n=1,blocksize=None,quiet=False,dtype=np.uint8):
    &#34;&#34;&#34;
    bins numpy ndarrays in arbitrary dimensions by a factor n. Prior to
    binning, elements from the end are deleted until the length is a
    multiple of the bin factor. Executes averaging of bins in floating
    point precision, which is memory intensive for large stacks. Using
    smaller blocks reduces memory usage, but is less efficient.
    Parameters
    ----------
    images : numpy.ndarray
        ndarray containing the data
    n : int or tuple of int, optional
        factor to bin with for all dimensions (int) or each dimension
        individually (tuple with one int per dimension). The default is 1.
    blocksize : int, optional
        number of (binned) slices to process at a time to conserve memory.
        The default is entire stack.
    quiet : bool, optional
        suppresses printed output when True. The default is False.
    dtype : (numpy) datatype, optional
        datatype to use for output. Averaging of the binned pixels always
        occurs in floating point precision. The default is np.uint8.
    Returns
    -------
    images : numpy.ndarray
        binned stack
    &#34;&#34;&#34;
    dims = images.ndim
    
    #check input parameters for type
    if type(n) != tuple and type(n) != int:
        print(&#39;n must be int or tuple of ints, skipping binning step&#39;)
        return images
    
    #when n is int, convert to tuple of ints with length dims
    if type(n) == int:
        n = (n,)*dims
        
    #else if n is a tuple, check if length of n matches dims
    elif len(n) != dims:
        print(&#39;number of dimensions does not match, skipping binning step&#39;)
        return images
    
    #skip rest of code when every element in n is 1
    if all([nitem==1 for nitem in n]):
        if not quiet:
            print(&#39;no binning used&#39;)
        return images

    #define new shapes
    oldshape = np.shape(images)
    trimmedshape = tuple([int(oldshape[d] - oldshape[d] % n[d]) for d in range(dims)])
    newshape = tuple([int(trimmedshape[d]/n[d]) for d in range(dims)])
    
    #trim ends when new shape is not a whole multiple of binfactor
    slices = [range(trimmedshape[d],oldshape[d]) for d in range(dims)]
    for d in range(dims):
        if trimmedshape[d] != oldshape[d]:
            images = np.delete(images,slices[d],axis=d)
    
    #print old and new shapes when trimming and or binning is used
    if oldshape != trimmedshape and not quiet:
        print(&#39;trimming from shape {} to {}, binning to {}...&#39;.format(oldshape,trimmedshape,newshape))
    elif not quiet:
        print(&#39;binning from shape {} to {}...&#39;.format(trimmedshape,newshape))
    
    #check block size
    if blocksize == None:
        blocksize = newshape[0]
    
    #determine shape such that reshaped array has dims*2 dimensions, with each dim spread over new axis to bin along
    reshapeshape = [i for subtuple in [(newshape[d],n[d]) for d in range(dims)] for i in subtuple]
    
    #when binning entire array at once
    if blocksize &gt;= newshape[0]:
        #execute reshape
        images = images.reshape(reshapeshape)
        
        #average along binning axes to obtain original dimensionality
        #(floating point precision and memory intensive!!)
        for d in reversed(range(1,2*dims,2)):
            images = images.mean(d)
        
        #set type back from floating point
        images = images.astype(dtype)
        
    #when splitting binning into multiple blocks to conserve memory
    else:
        if not quiet:
            print(&#39;splitting binning in {} blocks&#39;.format(int(np.ceil(newshape[0]/blocksize))))
        
        #reshape stack and store to temp variable
        reshapeim = images.reshape(reshapeshape)
        
        #overwrite images with empty array of new shape
        images = np.zeros(newshape,dtype=dtype)
        
        #perform binning in steps of blocksize
        for i in range(0,newshape[0] - (newshape[0] % blocksize),blocksize):
            block = reshapeim[i:i+blocksize]
            for d in reversed(range(1,2*dims,2)):
                block = np.mean(block,axis=d)
            images[i:i+blocksize] = block.astype(dtype)
        
        #execute last (smaller) block if newshape is not devisible by blocksize
        if newshape[0] % blocksize != 0:
            block = reshapeim[-(newshape[0] % blocksize):]
            for d in reversed(range(1,2*dims,2)):
                block = np.mean(block,axis=d)
            images[-(newshape[0] % blocksize):] = block.astype(dtype)
    
    if not quiet:
        print(&#39;binning finished&#39;)
    
    return images</code></pre>
</details>
</dd>
<dt id="scm_confocal.util.fit_powerlaw"><code class="name flex">
<span>def <span class="ident">fit_powerlaw</span></span>(<span>x, y, weights=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Linear regression in log space of the MSD to get diffusion constant, which
is a powerlaw in linear space of the form A<em>x</em>*n
Parameters</p>
<hr>
<dl>
<dt><strong><code>x</code></strong> :&ensp;<code>list</code> or <code>numpy.array</code></dt>
<dd>x coordinates of data points to fit</dd>
<dt><strong><code>y</code></strong> :&ensp;<code>list</code> or <code>numpy.array</code></dt>
<dd>y coordinates of data points to fit</dd>
<dt><strong><code>weights</code></strong> :&ensp;<code>list</code> or <code>numpy.array</code>, optional</dt>
<dd>list of weights to use for each (x,y) coordinate. The default is
None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>A</code></strong> :&ensp;<code>float</code></dt>
<dd>constant A</dd>
<dt><strong><code>n</code></strong> :&ensp;<code>float</code></dt>
<dd>power n</dd>
<dt><strong><code>sigmaA</code></strong> :&ensp;<code>float</code></dt>
<dd>standard deviation in A</dd>
<dt><strong><code>sigmaN</code></strong> :&ensp;<code>float</code></dt>
<dd>standard deviation in n</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit_powerlaw(x,y,weights=None):
    &#34;&#34;&#34;
    Linear regression in log space of the MSD to get diffusion constant, which
    is a powerlaw in linear space of the form A*x**n
    Parameters
    ----------
    x : list or numpy.array
        x coordinates of data points to fit
    y : list or numpy.array
        y coordinates of data points to fit
    weights : list or numpy.array, optional
        list of weights to use for each (x,y) coordinate. The default is 
        None.
    Returns
    -------
    A : float
        constant A
    n : float
        power n
    sigmaA : float
        standard deviation in A
    sigmaN : float
        standard deviation in n
    &#34;&#34;&#34;
    import scipy.optimize
    
    def f(x,a,b):
        return a*x + b
    
    if weights is None:
        weights = np.ones(len(y))
    else:
        weights = np.array(weights)
    
    #remove nan values
    x = np.array(x)
    y = np.array(y)
    
    x = x[~np.isnan(y)]
    weights = weights[~np.isnan(y)]
    y = y[~np.isnan(y)]
    
    #fit
    (n,A), covariance = scipy.optimize.curve_fit(f,np.log(x),np.log(y),sigma=weights)
    sigmaN,sigmaA = np.sqrt(np.diag(covariance))
    A = np.exp(A)
    sigmaA = sigmaA*np.exp(A)
    
    return A,n,sigmaA,sigmaN</code></pre>
</details>
</dd>
<dt id="scm_confocal.util.mean_square_displacement"><code class="name flex">
<span>def <span class="ident">mean_square_displacement</span></span>(<span>features, pos_cols=['x', 'y', 'z'], t_col='t (s)', nparticles=None, pickrandom=False, nbins=20, tmin=None, tmax=None)</span>
</code></dt>
<dd>
<section class="desc"><p>calculate the mean square displacement vs time for linked particles
Parameters</p>
<hr>
<dl>
<dt><strong><code>features</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>output from trackpy.link containing tracking data</dd>
<dt><strong><code>pos_cols</code></strong> :&ensp;<code>list</code> of <code>str</code>, optional</dt>
<dd>names of columns to use for coordinates. The default is
['x','y','z'].</dd>
<dt><strong><code>t_col</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>name of column containing timestamps. The default is 't (s)'.</dd>
<dt><strong><code>nparticles</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of particles to use for calculation (useful for large
datasets). The default is all particles.</dd>
<dt><strong><code>pickrandom</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>whether to pick nparticles randomly or not, if False it takes the
n longest tracked particles from data. The default is False.</dd>
<dt><strong><code>nbins</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of bins for output. The default is 20.</dd>
<dt><strong><code>tmin</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>left edge of first bin. The default is min(t_col).</dd>
<dt><strong><code>tmax</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>&nbsp;</dd>
</dl>
<p>right edge of last bin, The default is max(t_col).
Returns</p>
<hr>
<dl>
<dt><strong><code>binedges</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>edges of time bins</dd>
<dt><strong><code>bincounts</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>number of sampling points for each bin</dd>
<dt><strong><code>binmeans</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>mean square displacement values</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_square_displacement(features, pos_cols = [&#39;x&#39;,&#39;y&#39;,&#39;z&#39;], t_col=&#39;t (s)&#39;,
                         nparticles=None, pickrandom=False, nbins=20,
                         tmin=None, tmax=None):
    &#34;&#34;&#34;
    calculate the mean square displacement vs time for linked particles
    Parameters
    ----------
    features : pandas.DataFrame
        output from trackpy.link containing tracking data
    pos_cols : list of str, optional
        names of columns to use for coordinates. The default is
        [&#39;x&#39;,&#39;y&#39;,&#39;z&#39;].
    t_col : str, optional
        name of column containing timestamps. The default is &#39;t (s)&#39;.
    nparticles : int, optional
        number of particles to use for calculation (useful for large
        datasets). The default is all particles.
    pickrandom : bool, optional
        whether to pick nparticles randomly or not, if False it takes the 
        n longest tracked particles from data. The default is False.
    nbins : int, optional
        number of bins for output. The default is 20.
    tmin : float, optional
        left edge of first bin. The default is min(t_col).
    tmax : float, optional
       right edge of last bin, The default is max(t_col).
    Returns
    -------
    binedges : numpy.array
        edges of time bins
    bincounts : numpy.array
        number of sampling points for each bin
    binmeans : numpy.array
        mean square displacement values
    &#34;&#34;&#34;
    #restructure data to correct order
    features = features[[&#39;particle&#39;]+[t_col]+pos_cols]
    features = features.set_index(&#39;particle&#39;)
    dims = len(pos_cols)
    
    #initialize empty array to contain [[dt1,dr1],[dt2,dr2],...]
    dt_dr = np.empty((0,2))
    
    #converting to set assures unique values only
    particles = set(features.index)
    
    #optionally take a subset of particles
    if nparticles != None and len(particles)&gt;nparticles:
        
        #optionally take random subset of particles
        if pickrandom:
            import random
            particles = random.sample(set(features.index),nparticles)
        
        #else take the particles which occur in most of the frames
        else:
            vals, counts = np.unique(features.index, return_counts=True)
            sortedindices = np.argsort(-counts)[:nparticles]
            particles = vals[sortedindices]
    
    #iterate over all particles and all time intervals for that particle and 
    # append [[dr,dt]] to dt_dr each time
    for p in particles:
        pdata = features.loc[p]
        for j in range(len(pdata)):
            for i in range(j):
                dt_dr = np.append(
                        dt_dr,
                        [[
                                pdata.iat[j,0] - pdata.iat[i,0],
                                sum([(pdata.iat[j,d] - pdata.iat[i,d])**2 for d in range(1,dims+1)])
                        ]],
                        axis = 0
                        )
    
    #check bins
    if tmin == None:
        tmin = min(dt_dr[:,0])
    if tmax == None:
        tmax = max(dt_dr[:,0])
    
    #create bin edges
    binedges = np.linspace(tmin,tmax,nbins+1,endpoint=True)
    
    #put each timestep into the correct bin and remove data left of first bin
    binpos = binedges.searchsorted(dt_dr[:,0],side=&#39;right&#39;)
    dt_dr = dt_dr[binpos!=0]
    binpos = binpos[binpos!=0]
    binpos = binpos-1
    
    #count each bin and weigh counts by corresponding r, then normalize by unweighted counts
    bincounts = np.bincount(binpos, minlength=nbins)
    binmeans =  np.bincount(binpos, weights=dt_dr[:,1], minlength=nbins) / bincounts
    
    return binedges,bincounts[:-1],binmeans[:-1]</code></pre>
</details>
</dd>
<dt id="scm_confocal.util.mean_square_displacement_per_frame"><code class="name flex">
<span>def <span class="ident">mean_square_displacement_per_frame</span></span>(<span>features, pos_cols=['x', 'y'], feat_col='particle')</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate the mean square movement of all tracked features between
subsequent frames using efficient pandas linear algebra
Parameters</p>
<hr>
<dl>
<dt><strong><code>features</code></strong> :&ensp;<code>pandas.Dataframe</code></dt>
<dd>dataframe containing the tracking data over timesteps indexed by
frame number and containing coordinates of features.</dd>
<dt><strong><code>pos_cols</code></strong> :&ensp;<code>list</code> of <code>str</code>, optional</dt>
<dd>names of the columns containing coordinates. The default is
['x','y'].</dd>
<dt><strong><code>feat_col</code></strong> :&ensp;<code>str</code></dt>
<dd>name of column containing feature identifyers. The default is
'particle'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>msd</code></strong> :&ensp;<code>numpy.array</code></dt>
<dd>averages of the squared displacements between each two steps</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_square_displacement_per_frame(features, pos_cols = [&#39;x&#39;,&#39;y&#39;], feat_col = &#39;particle&#39;):
    &#34;&#34;&#34;
    Calculate the mean square movement of all tracked features between
    subsequent frames using efficient pandas linear algebra
    Parameters
    ----------
    features : pandas.Dataframe
        dataframe containing the tracking data over timesteps indexed by
        frame number and containing coordinates of features.
    pos_cols : list of str, optional
        names of the columns containing coordinates. The default is
        [&#39;x&#39;,&#39;y&#39;].
    feat_col : str
        name of column containing feature identifyers. The default is
        &#39;particle&#39;.
    Returns
    -------
    msd : numpy.array
        averages of the squared displacements between each two steps
    &#34;&#34;&#34;

    nf = int(max(features.index))
    
    features = features[[feat_col]+pos_cols]
    msd = np.empty((nf))
    
    #loop over all sets of subsequent frames 
    for i in range(nf):
        
        #create subsets for current and next frame
        a = features.loc[i].set_index(feat_col)
        b = features.loc[i+1].set_index(feat_col)
        
        #find all features which occur in both frames
        f = set(a.index).intersection(set(b.index))
        
        #join positional columns of particles to a single DataFrame
        a = a.loc[f][pos_cols].join(b.loc[f][pos_cols], rsuffix=&#39;_b&#39;)
        
        #create a new column with sum of squared displacement along each direction
        a[&#39;dr**2&#39;] = np.sum([(a[pos + &#39;_b&#39;] - a[pos])**2 for pos in pos_cols], 0)
        
        #take the mean of all square displacement column and add to list
        msd[i] = a[&#39;dr**2&#39;].mean()
    
    return msd</code></pre>
</details>
</dd>
<dt id="scm_confocal.util.multiply_intensity"><code class="name flex">
<span>def <span class="ident">multiply_intensity</span></span>(<span>data, factor, dtype=None)</span>
</code></dt>
<dd>
<section class="desc"><p>For multiplying the values of a numpy array while accounting for
integer overflow issues in integer datatypes. Corrected values larger
than the datatype max are set to the max value.
Parameters</p>
<hr>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>array containing the data values</dd>
<dt><strong><code>factor</code></strong> :&ensp;<code>float</code></dt>
<dd>factor to multiply data with</dd>
<dt><strong><code>dtype</code></strong> :&ensp;(<code>numpy</code>) <code>datatype</code>, optional</dt>
<dd>Datatype to scale data to. The default is the same type as the
input data.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>data with new intensity values.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def multiply_intensity(data,factor,dtype=None):
    &#34;&#34;&#34;
    For multiplying the values of a numpy array while accounting for
    integer overflow issues in integer datatypes. Corrected values larger
    than the datatype max are set to the max value.
    Parameters
    ----------
    data : numpy.ndarray
        array containing the data values
    factor : float
        factor to multiply data with
    dtype : (numpy) datatype, optional
        Datatype to scale data to. The default is the same type as the
        input data.
    Returns
    -------
    data : numpy.ndarray
        data with new intensity values.
    &#34;&#34;&#34;

    if factor == 1:
        return data
    
    if dtype == None:
        dtype = data.dtype
    
    #try if integer type, if not it must be float so one can just multiply
    try:
        maxval = np.iinfo(dtype).max
    except ValueError:
        return data*factor
    
    #for integer types, account for integer overflow
    data[data &gt;= maxval/factor] = maxval
    data[data &lt;  maxval/factor] = data[data &lt;  maxval/factor] * factor
    
    return data</code></pre>
</details>
</dd>
<dt id="scm_confocal.util.plot_stack_histogram"><code class="name flex">
<span>def <span class="ident">plot_stack_histogram</span></span>(<span>images, bin_edges=range(0, 256), newfig=True, legendname=None, title='intensity histogram')</span>
</code></dt>
<dd>
<section class="desc"><p>manually flattens list of images to list of pixel values and plots
histogram. Can combine multiple calls with newfig and legendname
options
Parameters</p>
<hr>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>numpy</code> <code>ndarray</code></dt>
<dd>array containing pixel values</dd>
<dt><strong><code>bin_edges</code></strong> :&ensp;<code>list</code> or <code>range</code>, optional</dt>
<dd>edges of bins to use. The default is range(0,256).</dd>
<dt><strong><code>newfig</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to open a new figure or to add to currently active figure.
The default is True.</dd>
<dt><strong><code>legendname</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>label to use for the legend. The default is None.</dd>
<dt><strong><code>title</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>text to use as plot title. The default is 'intensity histogram'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>pyplot</code> <code>figure</code> <code>handle</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_stack_histogram(images,bin_edges=range(0,256),newfig=True,legendname=None,title=&#39;intensity histogram&#39;):
    &#34;&#34;&#34;
    manually flattens list of images to list of pixel values and plots
    histogram. Can combine multiple calls with newfig and legendname
    options
    Parameters
    ----------
    images : numpy ndarray
        array containing pixel values
    bin_edges : list or range, optional
        edges of bins to use. The default is range(0,256).
    newfig : bool, optional
        Whether to open a new figure or to add to currently active figure.
        The default is True.
    legendname : string, optional
        label to use for the legend. The default is None.
    title : string, optional
        text to use as plot title. The default is &#39;intensity histogram&#39;.
    Returns
    -------
    pyplot figure handle
    &#34;&#34;&#34;
    from matplotlib import pyplot as plt
    
    if newfig:
        fig = plt.figure()
        plt.xlabel(&#39;grey value&#39;)
        plt.ylabel(&#39;counts&#39;)
        plt.title(title)
    else:
        fig = plt.gcf()
    
    plt.hist(np.ravel(images),log=(False,True),bins=bin_edges,label=legendname)
    
    if not legendname == None:
        plt.legend()
    plt.show()
    return fig</code></pre>
</details>
</dd>
<dt id="scm_confocal.util.saveprompt"><code class="name flex">
<span>def <span class="ident">saveprompt</span></span>(<span>question='Save/overwrite? 1=YES, 0=NO. ')</span>
</code></dt>
<dd>
<section class="desc"><p>aks user to save, returns boolean</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def saveprompt(question=&#34;Save/overwrite? 1=YES, 0=NO. &#34;):
    &#34;&#34;&#34;
    aks user to save, returns boolean
    &#34;&#34;&#34;
    try:
        savefile = int(input(question))
    except ValueError:
        savefile = 0
    if savefile&gt;1 or savefile&lt;0:
        savefile = 0
    if savefile==1:
         print(&#34;saving data&#34;)
         save = True
    else:
        print(&#34;not saving input parameters&#34;)
        save = False
    return save</code></pre>
</details>
</dd>
<dt id="scm_confocal.util.subtract_background"><code class="name flex">
<span>def <span class="ident">subtract_background</span></span>(<span>images, val=0, percentile=False)</span>
</code></dt>
<dd>
<section class="desc"><p>subtract a constant value from a numpy array without going below 0
Parameters</p>
<hr>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>numpy</code> <code>ndarray</code></dt>
<dd>images to correct.</dd>
<dt><strong><code>percentile</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to give the value as a percentile of the stack rather than
an absolute value to subtrackt. The default is False.</dd>
<dt><strong><code>val</code></strong> :&ensp;<code>int</code> or <code>float</code>, optional</dt>
<dd>Value or percentile to subtract. The default is 0.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>images</code></strong> :&ensp;<code>numpy</code> <code>ndarray</code></dt>
<dd>the corrected stack.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def subtract_background(images, val=0, percentile=False):
    &#34;&#34;&#34;
    subtract a constant value from a numpy array without going below 0
    Parameters
    ----------
    images : numpy ndarray
        images to correct.
    percentile : bool, optional
        Whether to give the value as a percentile of the stack rather than
        an absolute value to subtrackt. The default is False.
    val : int or float, optional
        Value or percentile to subtract. The default is 0.
    Returns
    -------
    images : numpy ndarray
        the corrected stack.
    &#34;&#34;&#34;

    #calculate percentile val
    if percentile:
        val = np.percentile(images,val)

    #correct intensity
    images[images&lt;val] = 0
    images[images&gt;=val] = images[images&gt;=val]-val

    return images</code></pre>
</details>
</dd>
<dt id="scm_confocal.util.write_textfile"><code class="name flex">
<span>def <span class="ident">write_textfile</span></span>(<span>params, filename='parameters.txt')</span>
</code></dt>
<dd>
<section class="desc"><p>stores parameter names and values in text file
Parameters</p>
<hr>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dictionary</code> of <code>name</code>:<code>value</code></dt>
<dd>the data to store</dd>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>file name to us for saving. The default is "parameters.txt".</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_textfile(params,filename=&#34;parameters.txt&#34;):
    &#34;&#34;&#34;
    stores parameter names and values in text file
    Parameters
    ----------
    params : dictionary of name:value
        the data to store
    filename : str, optional
        file name to us for saving. The default is &#34;parameters.txt&#34;.
    Returns
    -------
    None.
    &#34;&#34;&#34;

    with open(filename,&#39;w&#39;) as file:
        for key,val in params.items():
            file.write(str(key)+&#39; = &#39;+str(val)+&#39;\n&#39;)
    print(&#34;input parameters saved in&#34;,filename)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scm_confocal.visitech_faststack"><code class="flex name class">
<span>class <span class="ident">visitech_faststack</span></span>
<span>(</span><span>filename, zsize, zstep, zbacksteps, zstart=0, magnification=63, binning=1)</span>
</code></dt>
<dd>
<section class="desc"><p>functions for fast stacks taken with the custom MicroManager Visitech
driver, saved to multipage .ome.tiff files containing entire stack</p>
<p>initialize class (lazy-loads data)
Parameters</p>
<hr>
<dl>
<dt><strong><code>filenames</code></strong> :&ensp;<code>string</code></dt>
<dd>name of first ome.tiff file (extension optional)</dd>
<dt><strong><code>zsize</code></strong> :&ensp;<code>float</code></dt>
<dd>z size (in um) of stack (first im to last)</dd>
<dt><strong><code>zstep</code></strong> :&ensp;<code>float</code></dt>
<dd>step size in z</dd>
<dt><strong><code>zbacksteps</code></strong> :&ensp;<code>int</code></dt>
<dd>number of backwards steps in z direction after each stack</dd>
<dt><strong><code>zstart</code></strong> :&ensp;<code>float</code></dt>
<dd>actual height of bottom of stack/lowest slice. The default is 0.</dd>
<dt><strong><code>magnification</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>magnification of objective lens used. The default is 63.</dd>
<dt><strong><code>binning</code></strong> :&ensp;<code>int</code></dt>
<dd>binning factor performed at the detector level, e.g. in
MicroManager software, in XY</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class visitech_faststack:
    &#34;&#34;&#34;
    functions for fast stacks taken with the custom MicroManager Visitech 
    driver, saved to multipage .ome.tiff files containing entire stack
    &#34;&#34;&#34;

    def __init__(self,filename,zsize,zstep,zbacksteps,zstart=0,magnification=63,binning=1):
        &#34;&#34;&#34;
        initialize class (lazy-loads data)
        Parameters
        ----------
        filenames : string
            name of first ome.tiff file (extension optional)
        zsize : float
            z size (in um) of stack (first im to last)
        zstep : float
            step size in z
        zbacksteps : int
            number of backwards steps in z direction after each stack
        zstart : float
            actual height of bottom of stack/lowest slice. The default is 0.
        magnification : float, optional
            magnification of objective lens used. The default is 63.
        binning : int
            binning factor performed at the detector level, e.g. in
            MicroManager software, in XY
        &#34;&#34;&#34;

        self.filename = filename

        #lazy-load data using PIMS
        print(&#39;starting PIMS&#39;)
        self.datafile = pims.TiffStack(filename)
        print(&#39;PIMS initialized&#39;)

        #find logical sizes of data
        self.nf = len(self.datafile)
        self.nz = int((zsize - zsize % zstep)/zstep + 1)
        self.nt = self.nf//(self.nz + zbacksteps)
        self.backsteps = zbacksteps

        #find physical sizes of data
        self.binning = binning
        self.zsteps = np.linspace(zstart,zstart+zsize,self.nz,endpoint=True)
        self.pixelsize = (zstep,6.5/magnification*binning,6.5/magnification*binning)
        #Hamamatsu C11440-22CU has pixels of 6.5x6.5 um

    def load_data(self,indices=slice(None,None,None),dtype=np.uint16):
        &#34;&#34;&#34;
        load images from datafile into 3D numpy array
        Parameters
        ----------
        indices : slice object or list of ints, optional
            which images from tiffstack to load. The default is
            slice(None,None,None).
        Returns
        -------
        numpy.ndarray containing image data in dim order (im,y,x)
        &#34;&#34;&#34;
        if type(indices) == slice:
            indices = range(self.nf)[indices]

        data = np.array(self.datafile[indices])

        if not data.dtype == dtype:
            print(&#39;rescaling data to type&#39;,dtype)
            data = data/np.amax(data)*np.iinfo(dtype).max
            data = data.astype(dtype)

        return data

    def load_stack(self,dim_range={},dtype=np.uint16,remove_backsteps=True,offset=0):
        &#34;&#34;&#34;
        Load the data and reshape into 4D stack with the following dimension
        order: (&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;)
        
        For loading only part of the total dataset, the dim_range parameter can
        be used to specify a range along any of the dimensions. This will be
        more memory efficient than loading the entire stack and then discarding
        part of the data. For slicing along the x or y axis this is not
        possible and whole (xy) images must be loaded prior to discarding
        data outside the specified x or y axis range.
        Parameters
        ----------
        dim_range : dict, optional
            dict, with keys corresponding to channel/dimension labels as above
            and slice objects as values. This allows you to only load part of
            the data along any of the dimensions, such as only loading two
            time steps or a particular z-range. An example use for only taking
            time steps up to 5 and z-slice 20 to 30 would
            be:
                dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
            The default is {} which corresponds to the full file.
        dtype : (numpy) datatype, optional
            type to scale data to. The default is np.uint16.
        remove_backsteps : bool
            whether to discard the frames which were recorded on the backsteps
            downwards
        offset : int
            offset the indices by a constant number of frames in case the first
            im is not the first slice of the first stack
        Returns
        -------
        data : numpy.ndarray
            ndarray with the pixel values
        &#34;&#34;&#34;
        #account for offset errors in data recording
        if offset == 0:
            indices = np.reshape(range(self.nf),(self.nt,self.nz+self.backsteps))
        else:
            #in case of offset, lose one stack in total (~half at begin and half at end)
            nf = self.nf - (self.nz+self.backsteps)
            nt = self.nt - 1
            indices = np.reshape(range(offset,offset+nf),(nt,self.nz+self.backsteps))

        #remove backsteps from indices
        if remove_backsteps:
            indices = indices[:,:self.nz]

        #check dim_range items for faulty values
        for key in dim_range.keys():
            if type(key) != str or key not in [&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;]:
                print(&#34;[WARNING] confocal.visitech_faststack.load_stack: &#34;+
                          &#34;dimension &#39;&#34;+key+&#34;&#39; not present in data, ignoring &#34;+
                          &#34;this entry.&#34;)
                dim_range.pop(key)

        #warn for inefficient x and y trimming
        if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
            print(&#34;[WARNING] confocal.visitech_faststack.load_stack: Loading&#34;+
                  &#34; only part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
                  &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
                  &#34;into memory before discarding values outside of the &#34;+
                  &#34;slice range specified for the x-axis and/or y-axis. &#34;+
                  &#34;Other axes for which a range is specified will still &#34;+
                  &#34;be treated normally, avoiding unneccesary memory use.&#34;)

        #remove values outside of dim_range from indices
        if &#39;time&#39; in dim_range:
            indices = indices[dim_range[&#39;time&#39;]]
        if &#39;z-axis&#39; in dim_range:
            #assure backsteps cannot be removed this way
            if remove_backsteps:
                indices = indices[:,dim_range[&#39;z-axis&#39;]]
            else:
                backsteps = indices[:,self.nz:]
                indices = indices[:,dim_range[&#39;z-axis&#39;]]
                indices = np.concatenate((indices,backsteps),axis=1)

        #store image indices array for self.get_timestamps(load_stack_indices=True)
        self._stack_indices = indices

        #load and reshape data
        stack = self.load_data(indices=indices.ravel(),dtype=dtype)
        shape = (indices.shape[0],indices.shape[1],stack.shape[1],stack.shape[2])
        stack = stack.reshape(shape)

        #trim x and y axis
        if &#39;y-axis&#39; in dim_range:
            stack = stack[:,:,dim_range[&#39;y-axis&#39;]]
        if &#39;x-axis&#39; in dim_range:
            stack = stack[:,:,:,dim_range[&#39;x-axis&#39;]]

        return stack

    def yield_stack(self,dim_range={},dtype=np.uint16,remove_backsteps=True,offset=0):
        &#34;&#34;&#34;
        Lazy-load the data and reshape into 4D stack with the following
        dimension order: (&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;). Returns a
        generator which yields a z-stack for each call, which is loaded upon
        calling it.
        
        For loading only part of the total dataset, the dim_range parameter can
        be used to specify a range along any of the dimensions. This will be
        more memory efficient than loading the entire stack and then discarding
        part of the data. For slicing along the x or y axis this is not
        possible and whole (xy) images must be loaded prior to discarding
        data outside the specified x or y axis range.
        The shape of the stack can be accessed without loading data using the 
        stack_shape attribute after creating the yield_stack object.
        Parameters
        ----------
        dim_range : dict, optional
            dict, with keys corresponding to channel/dimension labels as above
            and slice objects as values. This allows you to only load part of
            the data along any of the dimensions, such as only loading two
            time steps or a particular z-range. An example use for only taking
            time steps up to 5 and z-slice 20 to 30 would
            be:
                dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
            The default is {} which corresponds to the full file.
        dtype : (numpy) datatype, optional
            type to scale data to. The default is np.uint16.
        remove_backsteps : bool
            whether to discard the frames which were recorded on the backsteps
            downwards
        offset : int
            offset the indices by a constant number of frames in case the first
            im is not the first slice of the first stack
        Returns
        -------
        zstack : iterable/generator yielding numpy.ndarray
            list of time steps, with for each time step a z-stack as np.ndarray
            with the pixel values
        &#34;&#34;&#34;
        #account for offset errors in data recording
        if offset == 0:
            indices = np.reshape(range(self.nf),(self.nt,self.nz+self.backsteps))
        else:
            #in case of offset, lose one stack in total (~half at begin and half at end)
            nf = self.nf - (self.nz+self.backsteps)
            nt = self.nt - 1
            indices = np.reshape(range(offset,offset+nf),(nt,self.nz+self.backsteps))


        #remove backsteps from indices
        if remove_backsteps:
            indices = indices[:,:self.nz]

        #check dim_range items for faulty values
        for key in dim_range.keys():
            if type(key) != str or key not in [&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;]:
                print(&#34;[WARNING] confocal.visitech_faststack.load_stack: &#34;+
                          &#34;dimension &#39;&#34;+key+&#34;&#39; not present in data, ignoring &#34;+
                          &#34;this entry.&#34;)
                dim_range.pop(key)

        #warn for inefficient x and y trimming
        if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
            print(&#34;[WARNING] confocal.visitech_faststack.load_stack: Loading&#34;+
                  &#34; only part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
                  &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
                  &#34;into memory before discarding values outside of the &#34;+
                  &#34;slice range specified for the x-axis and/or y-axis. &#34;+
                  &#34;Other axes for which a range is specified will still &#34;+
                  &#34;be treated normally, avoiding unneccesary memory use.&#34;)

        #remove values outside of dim_range from indices
        if &#39;time&#39; in dim_range:
            indices = indices[dim_range[&#39;time&#39;]]
        if &#39;z-axis&#39; in dim_range:
            #assure backsteps cannot be removed this way
            if remove_backsteps:
                indices = indices[:,dim_range[&#39;z-axis&#39;]]
            else:
                backsteps = indices[:,self.nz:]
                indices = indices[:,dim_range[&#39;z-axis&#39;]]
                indices = np.concatenate((indices,backsteps),axis=1)

        #store image indices array for self.get_timestamps(load_stack_indices=True)
        self._stack_indices = indices

        #store stack size as attribute
        self.stack_shape = indices.shape + self.datafile[0].shape
        if &#39;y-axis&#39; in dim_range:
            self.stack_shape = self.stack_shape[:2] + (len(range(self.stack_shape[2])[dim_range[&#39;y-axis&#39;]]),self.stack_shape[3])
        if &#39;x-axis&#39; in dim_range:
            self.stack_shape = self.stack_shape[:3] + (len(range(self.stack_shape[3])[dim_range[&#39;x-axis&#39;]]),)

        #generator loop over each time step in a inner function such that the
        #initialization is excecuted up to this point upon creation rather than
        #upon iteration over the loop
        def stack_iter():
            for zstack_indices in indices:
                zstack = self.load_data(indices=zstack_indices.ravel(),dtype=dtype)
                zstack = zstack.reshape(self.stack_shape[1:])
    
                #trim x and y axis
                if &#39;y-axis&#39; in dim_range:
                    zstack = zstack[:,dim_range[&#39;y-axis&#39;]]
                if &#39;x-axis&#39; in dim_range:
                    zstack = zstack[:,:,dim_range[&#39;x-axis&#39;]]

                yield zstack

        return stack_iter()

    def save_stack(self,data,filename_prefix=&#39;visitech_faststack&#39;,sequence_type=&#39;multipage&#39;):
        &#34;&#34;&#34;
        save stacks to tiff files
        Parameters
        ----------
        data : numpy ndarray with 3 or 4 dimensions
            image series pixel values with dimension order (z,y,x) or (t,z,y,x)
        filename_prefix : string, optional
            prefix to use for filename. The time/z-axis index is appended if
            relevant. The default is &#39;visitech_faststack&#39;.
        sequence_type : string, optional
            The way to store the data. The following options are available:
                * &#39;image_sequence&#39; : stores as a series of 2D images with time and or frame number appended
                * &#39;multipage&#39; : store all data in a single multipage tiff file
                * &#39;multipage_sequence&#39; : stores a multipage tiff file for each time step
            The default is &#39;multipage&#39;.
        Returns
        -------
        None, but writes file(s) to working directory.
        &#34;&#34;&#34;
        from PIL import Image
        
        shape = np.shape(data)
        
        #store as series of named 2D images
        if sequence_type == &#39;image_sequence&#39;:
            #for (t,z,y,x)
            if len(shape) == 4:
                for i,t in enumerate(data):
                    for j,im in enumerate(t):
                        filename = filename_prefix + &#39;_t{:03d}_z{:03d}.tif&#39;.format(i,j)
                        Image.fromarray(im).save(filename)
            #for (z,y,x)
            elif len(shape) == 3:
                for i,im in enumerate(data):
                    filename = filename_prefix + &#39;_z{:03d}.tif&#39;.format(i,j)
                    Image.fromarray(im).save(filename)  
            else:
                raise ValueError(&#39;data must be 3-dimensional (z,y,x) or 4-dimensional (t,z,y,x)&#39;)
            
        #store as single multipage tiff
        elif sequence_type == &#39;multipage&#39;:
            #for (t,z,y,x)
            if len(shape) == 4:
                data = [Image.fromarray(im) for _ in data for im in _]
                data[0].save(filename_prefix+&#39;.tif&#39;,append_images=data[1:],save_all=True,)
            #for (z,y,x)
            elif len(shape) == 3:
                data = [Image.fromarray(im) for im in data]
                data[0].save(filename_prefix+&#39;.tif&#39;,append_images=data[1:],save_all=True,)
            else:
                raise ValueError(&#39;data must be 3-dimensional (z,y,x) or 4-dimensional (t,z,y,x)&#39;)
            
        elif sequence_type == &#39;multipage_sequence&#39;:
            if len(shape) == 4:
                for i,t in enumerate(data):
                    t = [Image.fromarray(im) for im in t]
                    t[0].save(filename_prefix+&#39;_t{:03d}.tif&#39;.format(i),append_images=t[1:],save_all=True)
            elif len(shape) == 3:
                print(&#34;[WARNING] scm_confocal.faststack.save_stack(): &#39;multipage_sequence&#39; invalid sequence_type for 3-dimensional data. Saving as option &#39;multipage&#39; instead&#34;)
                data = [Image.fromarray(im) for im in data]
                data[0].save(filename_prefix+&#39;.tif&#39;,append_images=data[1:],save_all=True)
            else:
                raise ValueError(&#39;data must be 4-dimensional (t,z,y,x)&#39;)

        else:
            raise ValueError(&#34;invalid option for sequence_type: must be &#39;image_sequence&#39;, &#39;multipage&#39; or &#39;multipage_sequence&#39;&#34;)
    
    def _get_metadata_string(filename,read_from_end=True):
        &#34;&#34;&#34;reads out the raw metadata from a file&#34;&#34;&#34;

        import io

        if read_from_end:
            #open file
            with io.open(filename, &#39;r&#39;, errors=&#39;ignore&#39;,encoding=&#39;utf8&#39;) as file:

                #set number of characters to move at a time
                blocksize=2**12
                overlap = 6

                #set starting position
                block = &#39;&#39;
                file.seek(0,os.SEEK_END)
                here = file.tell()-overlap
                end = here + overlap
                file.seek(here, os.SEEK_SET)

                #move back until OME start tag is found, store end tag position
                while 0 &lt; here and &#39;&lt;?xml&#39; not in block:
                    delta = min(blocksize, here)
                    here -= delta
                    file.seek(here, os.SEEK_SET)
                    block = file.read(delta+overlap)
                    if &#39;&lt;/OME&#39; in block:
                        end = here+delta+overlap

                #read until end
                file.seek(here, os.SEEK_SET)
                metadata = file.read(end-here)

        #process from start of the file
        else:
            metadata = &#39;&#39;
            read=False
            with io.open(filename, &#39;r&#39;, errors=&#39;ignore&#39;,encoding=&#39;utf8&#39;) as file:
                #read file line by line to avoid loading too much into memory
                for line in file:
                    #start reading on start of OME tiff header, break at end tag
                    if &#39;&lt;OME&#39; in line:
                        read = True
                    if read:
                        metadata += line
                        if &#39;&lt;/OME&#39; in line:
                            break

        #cut off extra characters from end
        return metadata[metadata.find(&#39;&lt;?xml&#39;):metadata.find(&#39;&lt;/OME&gt;&#39;)+6]

    def get_metadata(self,read_from_end=True):
        &#34;&#34;&#34;
        loads OME metadata from visitech .ome.tif file and returns xml tree object
        Parameters
        ----------
        read_from_end : bool, optional
            Whether to look for the metadata from the end of the file.
            The default is True.
        Returns
        -------
        xml.etree.ElementTree
            formatted XML metadata. Can be indexed with
            xml_root.find(&#39;&lt;element name&gt;&#39;)
        &#34;&#34;&#34;
        import xml.etree.ElementTree as et

        metadata = visitech_faststack._get_metadata_string(self.filename)

        #remove specifications
        metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/OME/2013-06&#34;&#39;,&#39;&#39;)
        metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/SA/2013-06&#34;&#39;,&#39;&#39;)
        metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/OME/2015-01&#34;&#39;,&#39;&#39;)
        metadata = metadata.replace(&#39;xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:schemaLocation=&#34;http://www.openmicroscopy.org/Schemas/OME/2015-01 http://www.openmicroscopy.org/Schemas/OME/2015-01/ome.xsd&#34;&#39;,&#39;&#39;)

        self.metadata = et.fromstring(metadata)
        return self.metadata

    def get_timestamps(self,load_stack_indices=False):
        &#34;&#34;&#34;
        loads OME metadata from visitech .ome.tif file and returns timestamps
        
        Parameters
        ----------
        load_stack_indices : boolean
            if True, only returns timestamps from frames which were loaded
            at call to visitech_faststack.load_stack(), and using the same
            dimension order / stack shape
    
        Returns
        -------
        times : numpy (nd)array of floats
            list/stack of timestamps for each of the the frames in the data
        &#34;&#34;&#34;
        import re

        metadata = visitech_faststack._get_metadata_string(self.filename)

        times = re.findall(r&#39;DeltaT=&#34;([0-9]*\.[0-9]*)&#34;&#39;,metadata)
        times = np.array([float(t) for t in times])

        if load_stack_indices:
            try:
                indices = self._stack_indices
            except AttributeError:
                raise AttributeError(&#39;data must be loaded with &#39;+
                                  &#39;visitech_faststack.load_stack() prior to &#39;+
                                  &#39;calling visitech_faststack.get_timestamps()&#39;
                                  +&#39; with load_stack_indices=True&#39;)

            times = times[indices.ravel()].reshape(np.shape(indices))

        self.times = times
        return times

    def get_pixelsize(self):
        &#34;&#34;&#34;shortcut to get (z,y,x) pixelsize with unit&#34;&#34;&#34;
        return (self.pixelsize,&#39;Âµm&#39;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="scm_confocal.visitech_faststack.get_metadata"><code class="name flex">
<span>def <span class="ident">get_metadata</span></span>(<span>self, read_from_end=True)</span>
</code></dt>
<dd>
<section class="desc"><p>loads OME metadata from visitech .ome.tif file and returns xml tree object
Parameters</p>
<hr>
<dl>
<dt><strong><code>read_from_end</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to look for the metadata from the end of the file.
The default is True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xml.etree.ElementTree</code></dt>
<dd>formatted XML metadata. Can be indexed with
xml_root.find('<element name>')</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata(self,read_from_end=True):
    &#34;&#34;&#34;
    loads OME metadata from visitech .ome.tif file and returns xml tree object
    Parameters
    ----------
    read_from_end : bool, optional
        Whether to look for the metadata from the end of the file.
        The default is True.
    Returns
    -------
    xml.etree.ElementTree
        formatted XML metadata. Can be indexed with
        xml_root.find(&#39;&lt;element name&gt;&#39;)
    &#34;&#34;&#34;
    import xml.etree.ElementTree as et

    metadata = visitech_faststack._get_metadata_string(self.filename)

    #remove specifications
    metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/OME/2013-06&#34;&#39;,&#39;&#39;)
    metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/SA/2013-06&#34;&#39;,&#39;&#39;)
    metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/OME/2015-01&#34;&#39;,&#39;&#39;)
    metadata = metadata.replace(&#39;xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:schemaLocation=&#34;http://www.openmicroscopy.org/Schemas/OME/2015-01 http://www.openmicroscopy.org/Schemas/OME/2015-01/ome.xsd&#34;&#39;,&#39;&#39;)

    self.metadata = et.fromstring(metadata)
    return self.metadata</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_faststack.get_pixelsize"><code class="name flex">
<span>def <span class="ident">get_pixelsize</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>shortcut to get (z,y,x) pixelsize with unit</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pixelsize(self):
    &#34;&#34;&#34;shortcut to get (z,y,x) pixelsize with unit&#34;&#34;&#34;
    return (self.pixelsize,&#39;Âµm&#39;)</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_faststack.get_timestamps"><code class="name flex">
<span>def <span class="ident">get_timestamps</span></span>(<span>self, load_stack_indices=False)</span>
</code></dt>
<dd>
<section class="desc"><p>loads OME metadata from visitech .ome.tif file and returns timestamps</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>load_stack_indices</code></strong> :&ensp;<code>boolean</code></dt>
<dd>if True, only returns timestamps from frames which were loaded
at call to visitech_faststack.load_stack(), and using the same
dimension order / stack shape</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>times</code></strong> :&ensp;<code>numpy</code> (<code>nd</code>)<code>array</code> of <code>floats</code></dt>
<dd>list/stack of timestamps for each of the the frames in the data</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_timestamps(self,load_stack_indices=False):
    &#34;&#34;&#34;
    loads OME metadata from visitech .ome.tif file and returns timestamps
    
    Parameters
    ----------
    load_stack_indices : boolean
        if True, only returns timestamps from frames which were loaded
        at call to visitech_faststack.load_stack(), and using the same
        dimension order / stack shape

    Returns
    -------
    times : numpy (nd)array of floats
        list/stack of timestamps for each of the the frames in the data
    &#34;&#34;&#34;
    import re

    metadata = visitech_faststack._get_metadata_string(self.filename)

    times = re.findall(r&#39;DeltaT=&#34;([0-9]*\.[0-9]*)&#34;&#39;,metadata)
    times = np.array([float(t) for t in times])

    if load_stack_indices:
        try:
            indices = self._stack_indices
        except AttributeError:
            raise AttributeError(&#39;data must be loaded with &#39;+
                              &#39;visitech_faststack.load_stack() prior to &#39;+
                              &#39;calling visitech_faststack.get_timestamps()&#39;
                              +&#39; with load_stack_indices=True&#39;)

        times = times[indices.ravel()].reshape(np.shape(indices))

    self.times = times
    return times</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_faststack.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self, indices=slice(None, None, None), dtype=numpy.uint16)</span>
</code></dt>
<dd>
<section class="desc"><p>load images from datafile into 3D numpy array
Parameters</p>
<hr>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>slice</code> <code>object</code> or <code>list</code> of <code>ints</code>, optional</dt>
<dd>which images from tiffstack to load. The default is
slice(None,None,None).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code> <code>containing</code> <code>image</code> <code>data</code> <code>in</code> <code>dim</code> <code>order</code> (<code>im</code>,<code>y</code>,<code>x</code>)</dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(self,indices=slice(None,None,None),dtype=np.uint16):
    &#34;&#34;&#34;
    load images from datafile into 3D numpy array
    Parameters
    ----------
    indices : slice object or list of ints, optional
        which images from tiffstack to load. The default is
        slice(None,None,None).
    Returns
    -------
    numpy.ndarray containing image data in dim order (im,y,x)
    &#34;&#34;&#34;
    if type(indices) == slice:
        indices = range(self.nf)[indices]

    data = np.array(self.datafile[indices])

    if not data.dtype == dtype:
        print(&#39;rescaling data to type&#39;,dtype)
        data = data/np.amax(data)*np.iinfo(dtype).max
        data = data.astype(dtype)

    return data</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_faststack.load_stack"><code class="name flex">
<span>def <span class="ident">load_stack</span></span>(<span>self, dim_range={}, dtype=numpy.uint16, remove_backsteps=True, offset=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Load the data and reshape into 4D stack with the following dimension
order: ('time','z-axis','y-axis','x-axis')</p>
<p>For loading only part of the total dataset, the dim_range parameter can
be used to specify a range along any of the dimensions. This will be
more memory efficient than loading the entire stack and then discarding
part of the data. For slicing along the x or y axis this is not
possible and whole (xy) images must be loaded prior to discarding
data outside the specified x or y axis range.
Parameters</p>
<hr>
<dl>
<dt><strong><code>dim_range</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>dict, with keys corresponding to channel/dimension labels as above
and slice objects as values. This allows you to only load part of
the data along any of the dimensions, such as only loading two
time steps or a particular z-range. An example use for only taking
time steps up to 5 and z-slice 20 to 30 would
be:
dim_range={'time':slice(None,5), 'z-axis':slice(20,30)}.
The default is {} which corresponds to the full file.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;(<code>numpy</code>) <code>datatype</code>, optional</dt>
<dd>type to scale data to. The default is np.uint16.</dd>
<dt><strong><code>remove_backsteps</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to discard the frames which were recorded on the backsteps
downwards</dd>
<dt><strong><code>offset</code></strong> :&ensp;<code>int</code></dt>
<dd>offset the indices by a constant number of frames in case the first
im is not the first slice of the first stack</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>ndarray with the pixel values</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_stack(self,dim_range={},dtype=np.uint16,remove_backsteps=True,offset=0):
    &#34;&#34;&#34;
    Load the data and reshape into 4D stack with the following dimension
    order: (&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;)
    
    For loading only part of the total dataset, the dim_range parameter can
    be used to specify a range along any of the dimensions. This will be
    more memory efficient than loading the entire stack and then discarding
    part of the data. For slicing along the x or y axis this is not
    possible and whole (xy) images must be loaded prior to discarding
    data outside the specified x or y axis range.
    Parameters
    ----------
    dim_range : dict, optional
        dict, with keys corresponding to channel/dimension labels as above
        and slice objects as values. This allows you to only load part of
        the data along any of the dimensions, such as only loading two
        time steps or a particular z-range. An example use for only taking
        time steps up to 5 and z-slice 20 to 30 would
        be:
            dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
        The default is {} which corresponds to the full file.
    dtype : (numpy) datatype, optional
        type to scale data to. The default is np.uint16.
    remove_backsteps : bool
        whether to discard the frames which were recorded on the backsteps
        downwards
    offset : int
        offset the indices by a constant number of frames in case the first
        im is not the first slice of the first stack
    Returns
    -------
    data : numpy.ndarray
        ndarray with the pixel values
    &#34;&#34;&#34;
    #account for offset errors in data recording
    if offset == 0:
        indices = np.reshape(range(self.nf),(self.nt,self.nz+self.backsteps))
    else:
        #in case of offset, lose one stack in total (~half at begin and half at end)
        nf = self.nf - (self.nz+self.backsteps)
        nt = self.nt - 1
        indices = np.reshape(range(offset,offset+nf),(nt,self.nz+self.backsteps))

    #remove backsteps from indices
    if remove_backsteps:
        indices = indices[:,:self.nz]

    #check dim_range items for faulty values
    for key in dim_range.keys():
        if type(key) != str or key not in [&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;]:
            print(&#34;[WARNING] confocal.visitech_faststack.load_stack: &#34;+
                      &#34;dimension &#39;&#34;+key+&#34;&#39; not present in data, ignoring &#34;+
                      &#34;this entry.&#34;)
            dim_range.pop(key)

    #warn for inefficient x and y trimming
    if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
        print(&#34;[WARNING] confocal.visitech_faststack.load_stack: Loading&#34;+
              &#34; only part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
              &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
              &#34;into memory before discarding values outside of the &#34;+
              &#34;slice range specified for the x-axis and/or y-axis. &#34;+
              &#34;Other axes for which a range is specified will still &#34;+
              &#34;be treated normally, avoiding unneccesary memory use.&#34;)

    #remove values outside of dim_range from indices
    if &#39;time&#39; in dim_range:
        indices = indices[dim_range[&#39;time&#39;]]
    if &#39;z-axis&#39; in dim_range:
        #assure backsteps cannot be removed this way
        if remove_backsteps:
            indices = indices[:,dim_range[&#39;z-axis&#39;]]
        else:
            backsteps = indices[:,self.nz:]
            indices = indices[:,dim_range[&#39;z-axis&#39;]]
            indices = np.concatenate((indices,backsteps),axis=1)

    #store image indices array for self.get_timestamps(load_stack_indices=True)
    self._stack_indices = indices

    #load and reshape data
    stack = self.load_data(indices=indices.ravel(),dtype=dtype)
    shape = (indices.shape[0],indices.shape[1],stack.shape[1],stack.shape[2])
    stack = stack.reshape(shape)

    #trim x and y axis
    if &#39;y-axis&#39; in dim_range:
        stack = stack[:,:,dim_range[&#39;y-axis&#39;]]
    if &#39;x-axis&#39; in dim_range:
        stack = stack[:,:,:,dim_range[&#39;x-axis&#39;]]

    return stack</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_faststack.save_stack"><code class="name flex">
<span>def <span class="ident">save_stack</span></span>(<span>self, data, filename_prefix='visitech_faststack', sequence_type='multipage')</span>
</code></dt>
<dd>
<section class="desc"><p>save stacks to tiff files
Parameters</p>
<hr>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy</code> <code>ndarray</code> <code>with</code> <code>3</code> or <code>4</code> <code>dimensions</code></dt>
<dd>image series pixel values with dimension order (z,y,x) or (t,z,y,x)</dd>
<dt><strong><code>filename_prefix</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>prefix to use for filename. The time/z-axis index is appended if
relevant. The default is 'visitech_faststack'.</dd>
<dt><strong><code>sequence_type</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>The way to store the data. The following options are available:
* 'image_sequence' : stores as a series of 2D images with time and or frame number appended
* 'multipage' : store all data in a single multipage tiff file
* 'multipage_sequence' : stores a multipage tiff file for each time step
The default is 'multipage'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>None, but writes file(s) to working directory.</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_stack(self,data,filename_prefix=&#39;visitech_faststack&#39;,sequence_type=&#39;multipage&#39;):
    &#34;&#34;&#34;
    save stacks to tiff files
    Parameters
    ----------
    data : numpy ndarray with 3 or 4 dimensions
        image series pixel values with dimension order (z,y,x) or (t,z,y,x)
    filename_prefix : string, optional
        prefix to use for filename. The time/z-axis index is appended if
        relevant. The default is &#39;visitech_faststack&#39;.
    sequence_type : string, optional
        The way to store the data. The following options are available:
            * &#39;image_sequence&#39; : stores as a series of 2D images with time and or frame number appended
            * &#39;multipage&#39; : store all data in a single multipage tiff file
            * &#39;multipage_sequence&#39; : stores a multipage tiff file for each time step
        The default is &#39;multipage&#39;.
    Returns
    -------
    None, but writes file(s) to working directory.
    &#34;&#34;&#34;
    from PIL import Image
    
    shape = np.shape(data)
    
    #store as series of named 2D images
    if sequence_type == &#39;image_sequence&#39;:
        #for (t,z,y,x)
        if len(shape) == 4:
            for i,t in enumerate(data):
                for j,im in enumerate(t):
                    filename = filename_prefix + &#39;_t{:03d}_z{:03d}.tif&#39;.format(i,j)
                    Image.fromarray(im).save(filename)
        #for (z,y,x)
        elif len(shape) == 3:
            for i,im in enumerate(data):
                filename = filename_prefix + &#39;_z{:03d}.tif&#39;.format(i,j)
                Image.fromarray(im).save(filename)  
        else:
            raise ValueError(&#39;data must be 3-dimensional (z,y,x) or 4-dimensional (t,z,y,x)&#39;)
        
    #store as single multipage tiff
    elif sequence_type == &#39;multipage&#39;:
        #for (t,z,y,x)
        if len(shape) == 4:
            data = [Image.fromarray(im) for _ in data for im in _]
            data[0].save(filename_prefix+&#39;.tif&#39;,append_images=data[1:],save_all=True,)
        #for (z,y,x)
        elif len(shape) == 3:
            data = [Image.fromarray(im) for im in data]
            data[0].save(filename_prefix+&#39;.tif&#39;,append_images=data[1:],save_all=True,)
        else:
            raise ValueError(&#39;data must be 3-dimensional (z,y,x) or 4-dimensional (t,z,y,x)&#39;)
        
    elif sequence_type == &#39;multipage_sequence&#39;:
        if len(shape) == 4:
            for i,t in enumerate(data):
                t = [Image.fromarray(im) for im in t]
                t[0].save(filename_prefix+&#39;_t{:03d}.tif&#39;.format(i),append_images=t[1:],save_all=True)
        elif len(shape) == 3:
            print(&#34;[WARNING] scm_confocal.faststack.save_stack(): &#39;multipage_sequence&#39; invalid sequence_type for 3-dimensional data. Saving as option &#39;multipage&#39; instead&#34;)
            data = [Image.fromarray(im) for im in data]
            data[0].save(filename_prefix+&#39;.tif&#39;,append_images=data[1:],save_all=True)
        else:
            raise ValueError(&#39;data must be 4-dimensional (t,z,y,x)&#39;)

    else:
        raise ValueError(&#34;invalid option for sequence_type: must be &#39;image_sequence&#39;, &#39;multipage&#39; or &#39;multipage_sequence&#39;&#34;)</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_faststack.yield_stack"><code class="name flex">
<span>def <span class="ident">yield_stack</span></span>(<span>self, dim_range={}, dtype=numpy.uint16, remove_backsteps=True, offset=0)</span>
</code></dt>
<dd>
<section class="desc"><p>Lazy-load the data and reshape into 4D stack with the following
dimension order: ('time','z-axis','y-axis','x-axis'). Returns a
generator which yields a z-stack for each call, which is loaded upon
calling it.</p>
<p>For loading only part of the total dataset, the dim_range parameter can
be used to specify a range along any of the dimensions. This will be
more memory efficient than loading the entire stack and then discarding
part of the data. For slicing along the x or y axis this is not
possible and whole (xy) images must be loaded prior to discarding
data outside the specified x or y axis range.
The shape of the stack can be accessed without loading data using the
stack_shape attribute after creating the yield_stack object.
Parameters</p>
<hr>
<dl>
<dt><strong><code>dim_range</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>dict, with keys corresponding to channel/dimension labels as above
and slice objects as values. This allows you to only load part of
the data along any of the dimensions, such as only loading two
time steps or a particular z-range. An example use for only taking
time steps up to 5 and z-slice 20 to 30 would
be:
dim_range={'time':slice(None,5), 'z-axis':slice(20,30)}.
The default is {} which corresponds to the full file.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;(<code>numpy</code>) <code>datatype</code>, optional</dt>
<dd>type to scale data to. The default is np.uint16.</dd>
<dt><strong><code>remove_backsteps</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to discard the frames which were recorded on the backsteps
downwards</dd>
<dt><strong><code>offset</code></strong> :&ensp;<code>int</code></dt>
<dd>offset the indices by a constant number of frames in case the first
im is not the first slice of the first stack</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>zstack</code></strong> :&ensp;<code>iterable</code>/<code>generator</code> <code>yielding</code> <code>numpy.ndarray</code></dt>
<dd>list of time steps, with for each time step a z-stack as np.ndarray
with the pixel values</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yield_stack(self,dim_range={},dtype=np.uint16,remove_backsteps=True,offset=0):
    &#34;&#34;&#34;
    Lazy-load the data and reshape into 4D stack with the following
    dimension order: (&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;). Returns a
    generator which yields a z-stack for each call, which is loaded upon
    calling it.
    
    For loading only part of the total dataset, the dim_range parameter can
    be used to specify a range along any of the dimensions. This will be
    more memory efficient than loading the entire stack and then discarding
    part of the data. For slicing along the x or y axis this is not
    possible and whole (xy) images must be loaded prior to discarding
    data outside the specified x or y axis range.
    The shape of the stack can be accessed without loading data using the 
    stack_shape attribute after creating the yield_stack object.
    Parameters
    ----------
    dim_range : dict, optional
        dict, with keys corresponding to channel/dimension labels as above
        and slice objects as values. This allows you to only load part of
        the data along any of the dimensions, such as only loading two
        time steps or a particular z-range. An example use for only taking
        time steps up to 5 and z-slice 20 to 30 would
        be:
            dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
        The default is {} which corresponds to the full file.
    dtype : (numpy) datatype, optional
        type to scale data to. The default is np.uint16.
    remove_backsteps : bool
        whether to discard the frames which were recorded on the backsteps
        downwards
    offset : int
        offset the indices by a constant number of frames in case the first
        im is not the first slice of the first stack
    Returns
    -------
    zstack : iterable/generator yielding numpy.ndarray
        list of time steps, with for each time step a z-stack as np.ndarray
        with the pixel values
    &#34;&#34;&#34;
    #account for offset errors in data recording
    if offset == 0:
        indices = np.reshape(range(self.nf),(self.nt,self.nz+self.backsteps))
    else:
        #in case of offset, lose one stack in total (~half at begin and half at end)
        nf = self.nf - (self.nz+self.backsteps)
        nt = self.nt - 1
        indices = np.reshape(range(offset,offset+nf),(nt,self.nz+self.backsteps))


    #remove backsteps from indices
    if remove_backsteps:
        indices = indices[:,:self.nz]

    #check dim_range items for faulty values
    for key in dim_range.keys():
        if type(key) != str or key not in [&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;]:
            print(&#34;[WARNING] confocal.visitech_faststack.load_stack: &#34;+
                      &#34;dimension &#39;&#34;+key+&#34;&#39; not present in data, ignoring &#34;+
                      &#34;this entry.&#34;)
            dim_range.pop(key)

    #warn for inefficient x and y trimming
    if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
        print(&#34;[WARNING] confocal.visitech_faststack.load_stack: Loading&#34;+
              &#34; only part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
              &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
              &#34;into memory before discarding values outside of the &#34;+
              &#34;slice range specified for the x-axis and/or y-axis. &#34;+
              &#34;Other axes for which a range is specified will still &#34;+
              &#34;be treated normally, avoiding unneccesary memory use.&#34;)

    #remove values outside of dim_range from indices
    if &#39;time&#39; in dim_range:
        indices = indices[dim_range[&#39;time&#39;]]
    if &#39;z-axis&#39; in dim_range:
        #assure backsteps cannot be removed this way
        if remove_backsteps:
            indices = indices[:,dim_range[&#39;z-axis&#39;]]
        else:
            backsteps = indices[:,self.nz:]
            indices = indices[:,dim_range[&#39;z-axis&#39;]]
            indices = np.concatenate((indices,backsteps),axis=1)

    #store image indices array for self.get_timestamps(load_stack_indices=True)
    self._stack_indices = indices

    #store stack size as attribute
    self.stack_shape = indices.shape + self.datafile[0].shape
    if &#39;y-axis&#39; in dim_range:
        self.stack_shape = self.stack_shape[:2] + (len(range(self.stack_shape[2])[dim_range[&#39;y-axis&#39;]]),self.stack_shape[3])
    if &#39;x-axis&#39; in dim_range:
        self.stack_shape = self.stack_shape[:3] + (len(range(self.stack_shape[3])[dim_range[&#39;x-axis&#39;]]),)

    #generator loop over each time step in a inner function such that the
    #initialization is excecuted up to this point upon creation rather than
    #upon iteration over the loop
    def stack_iter():
        for zstack_indices in indices:
            zstack = self.load_data(indices=zstack_indices.ravel(),dtype=dtype)
            zstack = zstack.reshape(self.stack_shape[1:])

            #trim x and y axis
            if &#39;y-axis&#39; in dim_range:
                zstack = zstack[:,dim_range[&#39;y-axis&#39;]]
            if &#39;x-axis&#39; in dim_range:
                zstack = zstack[:,:,dim_range[&#39;x-axis&#39;]]

            yield zstack

    return stack_iter()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="scm_confocal.visitech_series"><code class="flex name class">
<span>class <span class="ident">visitech_series</span></span>
<span>(</span><span>filename, magnification=63, binning=1)</span>
</code></dt>
<dd>
<section class="desc"><p>functions for image series taken with the multi-D acquisition menue in
MicroManager with the Visitech saved to multipage .ome.tiff files. For the
custom fast stack sequence use visitech_faststack.</p>
<p>initialize class (lazy-loads data)
Parameters</p>
<hr>
<dl>
<dt><strong><code>filenames</code></strong> :&ensp;<code>string</code></dt>
<dd>name of first ome.tiff file (extension optional)</dd>
<dt><strong><code>magnification</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>magnification of objective lens used. The default is 63.</dd>
<dt><strong><code>binning</code></strong> :&ensp;<code>int</code></dt>
<dd>binning factor performed at the detector level, e.g. in
MicroManager software, in XY</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class visitech_series:
    &#34;&#34;&#34;
    functions for image series taken with the multi-D acquisition menue in 
    MicroManager with the Visitech saved to multipage .ome.tiff files. For the
    custom fast stack sequence use visitech_faststack.
    &#34;&#34;&#34;
    def __init__(self,filename,magnification=63,binning=1):
        &#34;&#34;&#34;
        initialize class (lazy-loads data)
        Parameters
        ----------
        filenames : string
            name of first ome.tiff file (extension optional)
        magnification : float, optional
            magnification of objective lens used. The default is 63.
        binning : int
            binning factor performed at the detector level, e.g. in
            MicroManager software, in XY
        &#34;&#34;&#34;

        self.filename = filename

        #lazy-load data using PIMS
        print(&#39;initializing visitech_series&#39;)
        self.datafile = pims.TiffStack(filename)

        #find logical sizes of data
        self.nf = len(self.datafile)

        #find physical sizes of data
        self.magnification = magnification
        self.binning = binning
        self._pixelsizeXY = 6.5/magnification*binning
        #Hamamatsu C11440-22CU has pixels of 6.5x6.5 um

    def load_data(self,indices=slice(None,None,None),dtype=np.uint16):
        &#34;&#34;&#34;
        load images from datafile into 3D numpy array
        Parameters
        ----------
        indices : slice object or list of ints, optional
            which images from tiffstack to load. The default is
            slice(None,None,None).
        dtype : np int datatype
            data type / bit depth to rescale data to.
        Returns
        -------
        numpy.ndarray containing image data in dim order (im,y,x)
        &#34;&#34;&#34;
        if type(indices) == slice:
            indices = range(self.nf)[indices]

        data = np.array(self.datafile[indices])

        if not data.dtype == dtype:
            print(&#39;rescaling data to type&#39;,dtype)
            datamin = np.amin(data)
            data = (data-datamin)/(np.amax(data)-datamin)*np.iinfo(dtype).max
            data = data.astype(dtype)

        return data

    def load_stack(self,dim_range={},dtype=np.uint16):
        &#34;&#34;&#34;
        Load the data and reshape into 4D stack with the following dimension
        order: (&#39;channel&#39;,&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;) where dimensions
        with len 1 are omitted.
        
        For loading only part of the total dataset, the dim_range parameter can
        be used to specify a range along any of the dimensions. This will be
        more memory efficient than loading the entire stack and then discarding
        part of the data. For slicing along the x or y axis this is not
        possible and whole (xy) images must be loaded prior to discarding
        data outside the specified x or y axis range.
        Parameters
        ----------
        dim_range : dict, optional
            dict, with keys corresponding to channel/dimension labels as above
            and slice objects as values. This allows you to only load part of
            the data along any of the dimensions, such as only loading two
            time steps or a particular z-range. An example use for only taking
            time steps up to 5 and z-slice 20 to 30 would
            be:
                dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
            The default is {} which corresponds to the full file.
        dtype : (numpy) datatype, optional
            type to scale data to. The default is np.uint16.
        remove_backsteps : bool
            whether to discard the frames which were recorded on the backsteps
            downwards
        Returns
        -------
        data : numpy.ndarray
            ndarray with the pixel values
        &#34;&#34;&#34;
        #load the stack shape from metadata or reuse previous result
        try:
            self.shape
        except AttributeError:
            self.get_metadata_dimensions()

        #find shape and reshape indices
        shape = self.shape
        if &#39;x-axis&#39; in self.dimensions:
            shape = shape[:-1]
        if &#39;y-axis&#39; in self.dimensions:
            shape = shape[:-1]

        indices = np.reshape(range(self.nf),shape)

        #check dim_range items for faulty values
        for key in dim_range.keys():
            if type(key) != str or key not in self.dimensions:
                print(&#34;[WARNING] confocal.visitech_faststack.load_stack: &#34;+
                          &#34;dimension &#39;&#34;+key+&#34;&#39; not present in data, ignoring &#34;+
                          &#34;this entry.&#34;)
                dim_range.pop(key)

        #warn for inefficient x and y trimming
        if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
            print(&#34;[WARNING] confocal.visitech_faststack.load_stack: Loading&#34;+
                  &#34; only part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
                  &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
                  &#34;into memory before discarding values outside of the &#34;+
                  &#34;slice range specified for the x-axis and/or y-axis. &#34;+
                  &#34;Other axes for which a range is specified will still &#34;+
                  &#34;be treated normally, avoiding unneccesary memory use.&#34;)

        #remove values outside of dim_range from indices
        if &#39;time&#39; in dim_range:
            #this enumerate/tuple construction assures we slice the correct dim
            for i,dim in enumerate(self.dimensions):
                if dim==&#39;time&#39;:
                    indices = indices[(slice(None),)*i+(dim_range[&#39;time&#39;],)]
        if &#39;z-axis&#39; in dim_range:
            for i,dim in enumerate(self.dimensions):
                if dim==&#39;z-axis&#39;:
                    indices = indices[(slice(None),)*i+(dim_range[&#39;z-axis&#39;],)]

        #store image indices array for self.get_timestamps(load_stack_indices=True)
        self._stack_indices = indices

        #load and reshape data
        stack = self.load_data(indices=indices.ravel(),dtype=dtype)
        shape = indices.shape+stack.shape[-2:]
        stack = stack.reshape(shape)

        #trim x and y axis
        if &#39;y-axis&#39; in dim_range:
            for i,dim in enumerate(self.dimensions):
                if dim==&#39;y-axis&#39;:
                    stack = stack[(slice(None),)*i+(dim_range[&#39;y-axis&#39;],)]
        if &#39;x-axis&#39; in dim_range:
            for i,dim in enumerate(self.dimensions):
                if dim==&#39;x-axis&#39;:
                    stack = stack[(slice(None),)*i+(dim_range[&#39;x-axis&#39;],)]

        return stack

    def yield_stack(self,dim_range={},dtype=np.uint16,remove_backsteps=True):
        &#34;&#34;&#34;
        Lazy-load the data and reshape into 4D stack with the following
        dimension order: (&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;). Returns a
        generator which yields a z-stack for each call, which is loaded upon
        calling it.
        
        For loading only part of the total dataset, the dim_range parameter can
        be used to specify a range along any of the dimensions. This will be
        more memory efficient than loading the entire stack and then discarding
        part of the data. For slicing along the x or y axis this is not
        possible and whole (xy) images must be loaded prior to discarding
        data outside the specified x or y axis range.
        The shape of the stack can be accessed without loading data using the 
        stack_shape attribute after creating the yield_stack object.
        Parameters
        ----------
        dim_range : dict, optional
            dict, with keys corresponding to channel/dimension labels as above
            and slice objects as values. This allows you to only load part of
            the data along any of the dimensions, such as only loading two
            time steps or a particular z-range. An example use for only taking
            time steps up to 5 and z-slice 20 to 30 would
            be:
                dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
            The default is {} which corresponds to the full file.
        dtype : (numpy) datatype, optional
            type to scale data to. The default is np.uint16.
        remove_backsteps : bool
            whether to discard the frames which were recorded on the backsteps
            downwards
        Returns
        -------
        zstack : iterable/generator yielding numpy.ndarray
            list of time steps, with for each time step a z-stack as np.ndarray
            with the pixel values
        &#34;&#34;&#34;
        indices = np.reshape(range(self.nf),(self.nt,self.nz+self.backsteps))

        #remove backsteps from indices
        if remove_backsteps:
            indices = indices[:,:self.nz]

        #check dim_range items for faulty values
        for key in dim_range.keys():
            if type(key) != str or key not in [&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;]:
                print(&#34;[WARNING] confocal.visitech_faststack.load_stack: &#34;+
                          &#34;dimension &#39;&#34;+key+&#34;&#39; not present in data, ignoring &#34;+
                          &#34;this entry.&#34;)
                dim_range.pop(key)

        #warn for inefficient x and y trimming
        if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
            print(&#34;[WARNING] confocal.visitech_faststack.load_stack: Loading&#34;+
                  &#34; only part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
                  &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
                  &#34;into memory before discarding values outside of the &#34;+
                  &#34;slice range specified for the x-axis and/or y-axis. &#34;+
                  &#34;Other axes for which a range is specified will still &#34;+
                  &#34;be treated normally, avoiding unneccesary memory use.&#34;)

        #remove values outside of dim_range from indices
        if &#39;time&#39; in dim_range:
            indices = indices[dim_range[&#39;time&#39;]]
        if &#39;z-axis&#39; in dim_range:
            #assure backsteps cannot be removed this way
            if remove_backsteps:
                indices = indices[:,dim_range[&#39;z-axis&#39;]]
            else:
                backsteps = indices[:,self.nz:]
                indices = indices[:,dim_range[&#39;z-axis&#39;]]
                indices = np.concatenate((indices,backsteps),axis=1)

        #store image indices array for self.get_timestamps(load_stack_indices=True)
        self._stack_indices = indices

        #store stack size as attribute
        self.stack_shape = indices.shape + self.datafile[0].shape
        if &#39;y-axis&#39; in dim_range:
            self.stack_shape = self.stack_shape[:2] + (len(range(self.stack_shape[2])[dim_range[&#39;y-axis&#39;]]),self.stack_shape[3])
        if &#39;x-axis&#39; in dim_range:
            self.stack_shape = self.stack_shape[:3] + (len(range(self.stack_shape[3])[dim_range[&#39;x-axis&#39;]]),)

        #generator loop over each time step in a inner function such that the
        #initialization is excecuted up to this point upon creation rather than
        #upon iteration over the loop
        def stack_iter():
            for zstack_indices in indices:
                zstack = self.load_data(indices=zstack_indices.ravel(),dtype=dtype)
                zstack = zstack.reshape(self.stack_shape[1:])
    
                #trim x and y axis
                if &#39;y-axis&#39; in dim_range:
                    zstack = zstack[:,dim_range[&#39;y-axis&#39;]]
                if &#39;x-axis&#39; in dim_range:
                    zstack = zstack[:,:,dim_range[&#39;x-axis&#39;]]

                yield zstack

        return stack_iter()

    def _get_metadata_string(filename,read_from_end=True):
        &#34;&#34;&#34;reads out the raw metadata from a file&#34;&#34;&#34;

        import io

        if read_from_end:
            #open file
            with io.open(filename, &#39;r&#39;, errors=&#39;ignore&#39;,encoding=&#39;utf8&#39;) as file:

                #set number of characters to move at a time
                blocksize=2**12
                overlap = 6

                #set starting position
                block = &#39;&#39;
                file.seek(0,os.SEEK_END)
                here = file.tell()-overlap
                end = here + overlap
                file.seek(here, os.SEEK_SET)

                #move back until OME start tag is found, store end tag position
                while 0 &lt; here and &#39;&lt;?xml&#39; not in block:
                    delta = min(blocksize, here)
                    here -= delta
                    file.seek(here, os.SEEK_SET)
                    block = file.read(delta+overlap)
                    if &#39;&lt;/OME&#39; in block:
                        end = here+delta+overlap

                #read until end
                file.seek(here, os.SEEK_SET)
                metadata = file.read(end-here)

        #process from start of the file
        else:
            metadata = &#39;&#39;
            read=False
            with io.open(filename, &#39;r&#39;, errors=&#39;ignore&#39;,encoding=&#39;utf8&#39;) as file:
                #read file line by line to avoid loading too much into memory
                for line in file:
                    #start reading on start of OME tiff header, break at end tag
                    if &#39;&lt;OME&#39; in line:
                        read = True
                    if read:
                        metadata += line
                        if &#39;&lt;/OME&#39; in line:
                            break

        #cut off extra characters from end
        return metadata[metadata.find(&#39;&lt;?xml&#39;):metadata.find(&#39;&lt;/OME&gt;&#39;)+6]

    def get_metadata(self,read_from_end=True):
        &#34;&#34;&#34;
        loads OME metadata from visitech .ome.tif file and returns xml tree object
        Parameters
        ----------
        read_from_end : bool, optional
            Whether to look for the metadata from the end of the file.
            The default is True.
        Returns
        -------
        xml.etree.ElementTree
            formatted XML metadata. Can be indexed with
            xml_root.find(&#39;&lt;element name&gt;&#39;)
        &#34;&#34;&#34;
        import xml.etree.ElementTree as et

        metadata = visitech_faststack._get_metadata_string(self.filename)

        #remove specifications
        metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/OME/2013-06&#34;&#39;,&#39;&#39;)
        metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/SA/2013-06&#34;&#39;,&#39;&#39;)
        metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/OME/2015-01&#34;&#39;,&#39;&#39;)
        metadata = metadata.replace(&#39;xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:schemaLocation=&#34;http://www.openmicroscopy.org/Schemas/OME/2015-01 http://www.openmicroscopy.org/Schemas/OME/2015-01/ome.xsd&#34;&#39;,&#39;&#39;)
        print(metadata)
        self.metadata = et.fromstring(metadata)
        return self.metadata

    def get_metadata_dimensions(self):
        &#34;&#34;&#34;
        finds the stack&#39;s dimensionality and logical shape based on the
        embedded metadata
        Returns
        -------
        shape : tuple of ints
            logical sizes of the stack
        dimorder : tuple of strings
            order of the dimensions corresponding to the shape
        &#34;&#34;&#34;
        try:
            self.metadata
        except AttributeError:
            self.get_metadata()

        #get logical sizes from metadata
        dimorder_dict = dict(self.metadata.find(&#39;Image&#39;).find(&#39;Pixels&#39;).attrib)

        #check each dimension and append if present in dataset
        if dimorder_dict[&#39;SizeC&#39;] == &#39;1&#39;:
            dimorder = []
            shape = []
        else:
            dimorder = [&#39;channel&#39;]
            shape = [int(dimorder_dict[&#39;SizeC&#39;])]

        if dimorder_dict[&#39;SizeT&#39;] != &#39;1&#39;:
            dimorder.append(&#39;time&#39;)
            shape.append(int(dimorder_dict[&#39;SizeT&#39;]))

        if dimorder_dict[&#39;SizeZ&#39;] != &#39;1&#39;:
            dimorder.append(&#39;z-axis&#39;)
            shape.append(int(dimorder_dict[&#39;SizeZ&#39;]))

        if dimorder_dict[&#39;SizeY&#39;] != &#39;1&#39;:
            dimorder.append(&#39;y-axis&#39;)
            shape.append(int(dimorder_dict[&#39;SizeY&#39;]))

        if dimorder_dict[&#39;SizeX&#39;] != &#39;1&#39;:
            dimorder.append(&#39;x-axis&#39;)
            shape.append(int(dimorder_dict[&#39;SizeX&#39;]))

        shape = tuple(shape)
        dimorder = tuple(dimorder)

        self.shape = shape
        self.dimensions = dimorder

        return shape,dimorder

    def get_image_metadata(self,indices=slice(None)):
        &#34;&#34;&#34;
        loads the part of the metadata containing information about the time,
        position etc. for each frame of the series and returns a dataframe
        indexes by image frame
        Parameters
        ----------
        indices : slice object, optional
            which image frames to load the metadata for. The default is all
            frames.
        Returns
        -------
        imagedata : pandas.DataFrame
            the metadata for the images, indexed by frame number.
        &#34;&#34;&#34;
        import pandas as pd

        #load metadata (or use previous result if already loaded)
        try:
            self.metadata
        except AttributeError:
            self.get_metadata()

        #select part of the metadata with physical sizes for each image
        planedata = self.metadata.find(&#39;Image&#39;).find(&#39;Pixels&#39;).findall(&#39;Plane&#39;)

        imagedata = pd.DataFrame([dict(p.attrib) for p in planedata[indices]])
        imagedata = imagedata.astype({&#39;DeltaT&#39;:float,&#39;ExposureTime&#39;:float,&#39;PositionZ&#39;:float,&#39;TheC&#39;:int,&#39;TheT&#39;:int,&#39;TheZ&#39;:int})

        self.image_metadata = imagedata
        return imagedata

    def get_pixelsize(self):
        &#34;&#34;&#34;shortcut to get (z,y,x) pixelsize with unit&#34;&#34;&#34;
        try:
            self.dimensions
        except AttributeError:
            self.get_metadata_dimensions()

        pixelsize = []
        if &#39;z-axis&#39; in self.dimensions:
            pixelsize.append(float(dict(self.metadata.find(&#39;Image&#39;).find(&#39;Pixels&#39;).attrib)[&#39;PhysicalSizeZ&#39;]))
        if &#39;y-axis&#39; in self.dimensions:
            pixelsize.append(self._pixelsizeXY)
        if &#39;x-axis&#39; in self.dimensions:
            pixelsize.append(self._pixelsizeXY)

        self.pixelsize = pixelsize
        return (self.pixelsize,&#39;Âµm&#39;)

    def get_dimension_steps(self,dim,use_stack_indices=False):
        try:
            self.dimensions
        except AttributeError:
            self.get_metadata_dimensions()

        if dim not in self.dimensions or dim == &#39;channel&#39;:
            raise NotImplementedError(&#39;&#34;&#39;+dim+&#39;&#34; is not a valid dimension for visitech_series.get_dimension_steps()&#39;)

        if dim == &#39;time&#39;:
            if use_stack_indices:
                self.get_image_metadata(indices=self._stack_indices)
            else:
                try:
                    self.image_metadata
                except AttributeError:
                    self.get_image_metadata()
            return (np.array(self.image_metadata[&#39;DeltaT&#39;]),&#39;ms&#39;)

        if dim == &#39;z-axis&#39;:
            if use_stack_indices:
                self.get_image_metadata(indices=self._stack_indices)
            else:
                try:
                    self.image_metadata
                except AttributeError:
                    self.get_image_metadata()
            return (np.array(self.image_metadata[&#39;PositionZ&#39;]),&#39;Âµm&#39;)

        if dim == &#39;y-axis&#39;:
            if &#39;x-axis&#39; in self.dimensions:
                return (np.arange(0,self.shape[-2]*self._pixelsizeXY,self._pixelsizeXY),&#39;Âµm&#39;)
            else:
                return (np.arange(0,self.shape[-1]*self._pixelsizeXY,self._pixelsizeXY),&#39;Âµm&#39;)

        if dim == &#39;x-axis&#39;:
            return (np.arange(0,self.shape[-1]*self._pixelsizeXY,self._pixelsizeXY),&#39;Âµm&#39;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="scm_confocal.visitech_series.get_dimension_steps"><code class="name flex">
<span>def <span class="ident">get_dimension_steps</span></span>(<span>self, dim, use_stack_indices=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dimension_steps(self,dim,use_stack_indices=False):
    try:
        self.dimensions
    except AttributeError:
        self.get_metadata_dimensions()

    if dim not in self.dimensions or dim == &#39;channel&#39;:
        raise NotImplementedError(&#39;&#34;&#39;+dim+&#39;&#34; is not a valid dimension for visitech_series.get_dimension_steps()&#39;)

    if dim == &#39;time&#39;:
        if use_stack_indices:
            self.get_image_metadata(indices=self._stack_indices)
        else:
            try:
                self.image_metadata
            except AttributeError:
                self.get_image_metadata()
        return (np.array(self.image_metadata[&#39;DeltaT&#39;]),&#39;ms&#39;)

    if dim == &#39;z-axis&#39;:
        if use_stack_indices:
            self.get_image_metadata(indices=self._stack_indices)
        else:
            try:
                self.image_metadata
            except AttributeError:
                self.get_image_metadata()
        return (np.array(self.image_metadata[&#39;PositionZ&#39;]),&#39;Âµm&#39;)

    if dim == &#39;y-axis&#39;:
        if &#39;x-axis&#39; in self.dimensions:
            return (np.arange(0,self.shape[-2]*self._pixelsizeXY,self._pixelsizeXY),&#39;Âµm&#39;)
        else:
            return (np.arange(0,self.shape[-1]*self._pixelsizeXY,self._pixelsizeXY),&#39;Âµm&#39;)

    if dim == &#39;x-axis&#39;:
        return (np.arange(0,self.shape[-1]*self._pixelsizeXY,self._pixelsizeXY),&#39;Âµm&#39;)</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_series.get_image_metadata"><code class="name flex">
<span>def <span class="ident">get_image_metadata</span></span>(<span>self, indices=slice(None, None, None))</span>
</code></dt>
<dd>
<section class="desc"><p>loads the part of the metadata containing information about the time,
position etc. for each frame of the series and returns a dataframe
indexes by image frame
Parameters</p>
<hr>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>slice</code> <code>object</code>, optional</dt>
<dd>which image frames to load the metadata for. The default is all
frames.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>imagedata</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>the metadata for the images, indexed by frame number.</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_image_metadata(self,indices=slice(None)):
    &#34;&#34;&#34;
    loads the part of the metadata containing information about the time,
    position etc. for each frame of the series and returns a dataframe
    indexes by image frame
    Parameters
    ----------
    indices : slice object, optional
        which image frames to load the metadata for. The default is all
        frames.
    Returns
    -------
    imagedata : pandas.DataFrame
        the metadata for the images, indexed by frame number.
    &#34;&#34;&#34;
    import pandas as pd

    #load metadata (or use previous result if already loaded)
    try:
        self.metadata
    except AttributeError:
        self.get_metadata()

    #select part of the metadata with physical sizes for each image
    planedata = self.metadata.find(&#39;Image&#39;).find(&#39;Pixels&#39;).findall(&#39;Plane&#39;)

    imagedata = pd.DataFrame([dict(p.attrib) for p in planedata[indices]])
    imagedata = imagedata.astype({&#39;DeltaT&#39;:float,&#39;ExposureTime&#39;:float,&#39;PositionZ&#39;:float,&#39;TheC&#39;:int,&#39;TheT&#39;:int,&#39;TheZ&#39;:int})

    self.image_metadata = imagedata
    return imagedata</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_series.get_metadata"><code class="name flex">
<span>def <span class="ident">get_metadata</span></span>(<span>self, read_from_end=True)</span>
</code></dt>
<dd>
<section class="desc"><p>loads OME metadata from visitech .ome.tif file and returns xml tree object
Parameters</p>
<hr>
<dl>
<dt><strong><code>read_from_end</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether to look for the metadata from the end of the file.
The default is True.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>xml.etree.ElementTree</code></dt>
<dd>formatted XML metadata. Can be indexed with
xml_root.find('<element name>')</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata(self,read_from_end=True):
    &#34;&#34;&#34;
    loads OME metadata from visitech .ome.tif file and returns xml tree object
    Parameters
    ----------
    read_from_end : bool, optional
        Whether to look for the metadata from the end of the file.
        The default is True.
    Returns
    -------
    xml.etree.ElementTree
        formatted XML metadata. Can be indexed with
        xml_root.find(&#39;&lt;element name&gt;&#39;)
    &#34;&#34;&#34;
    import xml.etree.ElementTree as et

    metadata = visitech_faststack._get_metadata_string(self.filename)

    #remove specifications
    metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/OME/2013-06&#34;&#39;,&#39;&#39;)
    metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/SA/2013-06&#34;&#39;,&#39;&#39;)
    metadata = metadata.replace(&#39;xmlns=&#34;http://www.openmicroscopy.org/Schemas/OME/2015-01&#34;&#39;,&#39;&#39;)
    metadata = metadata.replace(&#39;xmlns:xsi=&#34;http://www.w3.org/2001/XMLSchema-instance&#34; xsi:schemaLocation=&#34;http://www.openmicroscopy.org/Schemas/OME/2015-01 http://www.openmicroscopy.org/Schemas/OME/2015-01/ome.xsd&#34;&#39;,&#39;&#39;)
    print(metadata)
    self.metadata = et.fromstring(metadata)
    return self.metadata</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_series.get_metadata_dimensions"><code class="name flex">
<span>def <span class="ident">get_metadata_dimensions</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>finds the stack's dimensionality and logical shape based on the
embedded metadata
Returns</p>
<hr>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>tuple</code> of <code>ints</code></dt>
<dd>logical sizes of the stack</dd>
<dt><strong><code>dimorder</code></strong> :&ensp;<code>tuple</code> of <code>strings</code></dt>
<dd>order of the dimensions corresponding to the shape</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_metadata_dimensions(self):
    &#34;&#34;&#34;
    finds the stack&#39;s dimensionality and logical shape based on the
    embedded metadata
    Returns
    -------
    shape : tuple of ints
        logical sizes of the stack
    dimorder : tuple of strings
        order of the dimensions corresponding to the shape
    &#34;&#34;&#34;
    try:
        self.metadata
    except AttributeError:
        self.get_metadata()

    #get logical sizes from metadata
    dimorder_dict = dict(self.metadata.find(&#39;Image&#39;).find(&#39;Pixels&#39;).attrib)

    #check each dimension and append if present in dataset
    if dimorder_dict[&#39;SizeC&#39;] == &#39;1&#39;:
        dimorder = []
        shape = []
    else:
        dimorder = [&#39;channel&#39;]
        shape = [int(dimorder_dict[&#39;SizeC&#39;])]

    if dimorder_dict[&#39;SizeT&#39;] != &#39;1&#39;:
        dimorder.append(&#39;time&#39;)
        shape.append(int(dimorder_dict[&#39;SizeT&#39;]))

    if dimorder_dict[&#39;SizeZ&#39;] != &#39;1&#39;:
        dimorder.append(&#39;z-axis&#39;)
        shape.append(int(dimorder_dict[&#39;SizeZ&#39;]))

    if dimorder_dict[&#39;SizeY&#39;] != &#39;1&#39;:
        dimorder.append(&#39;y-axis&#39;)
        shape.append(int(dimorder_dict[&#39;SizeY&#39;]))

    if dimorder_dict[&#39;SizeX&#39;] != &#39;1&#39;:
        dimorder.append(&#39;x-axis&#39;)
        shape.append(int(dimorder_dict[&#39;SizeX&#39;]))

    shape = tuple(shape)
    dimorder = tuple(dimorder)

    self.shape = shape
    self.dimensions = dimorder

    return shape,dimorder</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_series.get_pixelsize"><code class="name flex">
<span>def <span class="ident">get_pixelsize</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>shortcut to get (z,y,x) pixelsize with unit</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_pixelsize(self):
    &#34;&#34;&#34;shortcut to get (z,y,x) pixelsize with unit&#34;&#34;&#34;
    try:
        self.dimensions
    except AttributeError:
        self.get_metadata_dimensions()

    pixelsize = []
    if &#39;z-axis&#39; in self.dimensions:
        pixelsize.append(float(dict(self.metadata.find(&#39;Image&#39;).find(&#39;Pixels&#39;).attrib)[&#39;PhysicalSizeZ&#39;]))
    if &#39;y-axis&#39; in self.dimensions:
        pixelsize.append(self._pixelsizeXY)
    if &#39;x-axis&#39; in self.dimensions:
        pixelsize.append(self._pixelsizeXY)

    self.pixelsize = pixelsize
    return (self.pixelsize,&#39;Âµm&#39;)</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_series.load_data"><code class="name flex">
<span>def <span class="ident">load_data</span></span>(<span>self, indices=slice(None, None, None), dtype=numpy.uint16)</span>
</code></dt>
<dd>
<section class="desc"><p>load images from datafile into 3D numpy array
Parameters</p>
<hr>
<dl>
<dt><strong><code>indices</code></strong> :&ensp;<code>slice</code> <code>object</code> or <code>list</code> of <code>ints</code>, optional</dt>
<dd>which images from tiffstack to load. The default is
slice(None,None,None).</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>np</code> <code>int</code> <code>datatype</code></dt>
<dd>data type / bit depth to rescale data to.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code> <code>containing</code> <code>image</code> <code>data</code> <code>in</code> <code>dim</code> <code>order</code> (<code>im</code>,<code>y</code>,<code>x</code>)</dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_data(self,indices=slice(None,None,None),dtype=np.uint16):
    &#34;&#34;&#34;
    load images from datafile into 3D numpy array
    Parameters
    ----------
    indices : slice object or list of ints, optional
        which images from tiffstack to load. The default is
        slice(None,None,None).
    dtype : np int datatype
        data type / bit depth to rescale data to.
    Returns
    -------
    numpy.ndarray containing image data in dim order (im,y,x)
    &#34;&#34;&#34;
    if type(indices) == slice:
        indices = range(self.nf)[indices]

    data = np.array(self.datafile[indices])

    if not data.dtype == dtype:
        print(&#39;rescaling data to type&#39;,dtype)
        datamin = np.amin(data)
        data = (data-datamin)/(np.amax(data)-datamin)*np.iinfo(dtype).max
        data = data.astype(dtype)

    return data</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_series.load_stack"><code class="name flex">
<span>def <span class="ident">load_stack</span></span>(<span>self, dim_range={}, dtype=numpy.uint16)</span>
</code></dt>
<dd>
<section class="desc"><p>Load the data and reshape into 4D stack with the following dimension
order: ('channel','time','z-axis','y-axis','x-axis') where dimensions
with len 1 are omitted.</p>
<p>For loading only part of the total dataset, the dim_range parameter can
be used to specify a range along any of the dimensions. This will be
more memory efficient than loading the entire stack and then discarding
part of the data. For slicing along the x or y axis this is not
possible and whole (xy) images must be loaded prior to discarding
data outside the specified x or y axis range.
Parameters</p>
<hr>
<dl>
<dt><strong><code>dim_range</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>dict, with keys corresponding to channel/dimension labels as above
and slice objects as values. This allows you to only load part of
the data along any of the dimensions, such as only loading two
time steps or a particular z-range. An example use for only taking
time steps up to 5 and z-slice 20 to 30 would
be:
dim_range={'time':slice(None,5), 'z-axis':slice(20,30)}.
The default is {} which corresponds to the full file.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;(<code>numpy</code>) <code>datatype</code>, optional</dt>
<dd>type to scale data to. The default is np.uint16.</dd>
<dt><strong><code>remove_backsteps</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to discard the frames which were recorded on the backsteps
downwards</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>ndarray with the pixel values</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_stack(self,dim_range={},dtype=np.uint16):
    &#34;&#34;&#34;
    Load the data and reshape into 4D stack with the following dimension
    order: (&#39;channel&#39;,&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;) where dimensions
    with len 1 are omitted.
    
    For loading only part of the total dataset, the dim_range parameter can
    be used to specify a range along any of the dimensions. This will be
    more memory efficient than loading the entire stack and then discarding
    part of the data. For slicing along the x or y axis this is not
    possible and whole (xy) images must be loaded prior to discarding
    data outside the specified x or y axis range.
    Parameters
    ----------
    dim_range : dict, optional
        dict, with keys corresponding to channel/dimension labels as above
        and slice objects as values. This allows you to only load part of
        the data along any of the dimensions, such as only loading two
        time steps or a particular z-range. An example use for only taking
        time steps up to 5 and z-slice 20 to 30 would
        be:
            dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
        The default is {} which corresponds to the full file.
    dtype : (numpy) datatype, optional
        type to scale data to. The default is np.uint16.
    remove_backsteps : bool
        whether to discard the frames which were recorded on the backsteps
        downwards
    Returns
    -------
    data : numpy.ndarray
        ndarray with the pixel values
    &#34;&#34;&#34;
    #load the stack shape from metadata or reuse previous result
    try:
        self.shape
    except AttributeError:
        self.get_metadata_dimensions()

    #find shape and reshape indices
    shape = self.shape
    if &#39;x-axis&#39; in self.dimensions:
        shape = shape[:-1]
    if &#39;y-axis&#39; in self.dimensions:
        shape = shape[:-1]

    indices = np.reshape(range(self.nf),shape)

    #check dim_range items for faulty values
    for key in dim_range.keys():
        if type(key) != str or key not in self.dimensions:
            print(&#34;[WARNING] confocal.visitech_faststack.load_stack: &#34;+
                      &#34;dimension &#39;&#34;+key+&#34;&#39; not present in data, ignoring &#34;+
                      &#34;this entry.&#34;)
            dim_range.pop(key)

    #warn for inefficient x and y trimming
    if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
        print(&#34;[WARNING] confocal.visitech_faststack.load_stack: Loading&#34;+
              &#34; only part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
              &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
              &#34;into memory before discarding values outside of the &#34;+
              &#34;slice range specified for the x-axis and/or y-axis. &#34;+
              &#34;Other axes for which a range is specified will still &#34;+
              &#34;be treated normally, avoiding unneccesary memory use.&#34;)

    #remove values outside of dim_range from indices
    if &#39;time&#39; in dim_range:
        #this enumerate/tuple construction assures we slice the correct dim
        for i,dim in enumerate(self.dimensions):
            if dim==&#39;time&#39;:
                indices = indices[(slice(None),)*i+(dim_range[&#39;time&#39;],)]
    if &#39;z-axis&#39; in dim_range:
        for i,dim in enumerate(self.dimensions):
            if dim==&#39;z-axis&#39;:
                indices = indices[(slice(None),)*i+(dim_range[&#39;z-axis&#39;],)]

    #store image indices array for self.get_timestamps(load_stack_indices=True)
    self._stack_indices = indices

    #load and reshape data
    stack = self.load_data(indices=indices.ravel(),dtype=dtype)
    shape = indices.shape+stack.shape[-2:]
    stack = stack.reshape(shape)

    #trim x and y axis
    if &#39;y-axis&#39; in dim_range:
        for i,dim in enumerate(self.dimensions):
            if dim==&#39;y-axis&#39;:
                stack = stack[(slice(None),)*i+(dim_range[&#39;y-axis&#39;],)]
    if &#39;x-axis&#39; in dim_range:
        for i,dim in enumerate(self.dimensions):
            if dim==&#39;x-axis&#39;:
                stack = stack[(slice(None),)*i+(dim_range[&#39;x-axis&#39;],)]

    return stack</code></pre>
</details>
</dd>
<dt id="scm_confocal.visitech_series.yield_stack"><code class="name flex">
<span>def <span class="ident">yield_stack</span></span>(<span>self, dim_range={}, dtype=numpy.uint16, remove_backsteps=True)</span>
</code></dt>
<dd>
<section class="desc"><p>Lazy-load the data and reshape into 4D stack with the following
dimension order: ('time','z-axis','y-axis','x-axis'). Returns a
generator which yields a z-stack for each call, which is loaded upon
calling it.</p>
<p>For loading only part of the total dataset, the dim_range parameter can
be used to specify a range along any of the dimensions. This will be
more memory efficient than loading the entire stack and then discarding
part of the data. For slicing along the x or y axis this is not
possible and whole (xy) images must be loaded prior to discarding
data outside the specified x or y axis range.
The shape of the stack can be accessed without loading data using the
stack_shape attribute after creating the yield_stack object.
Parameters</p>
<hr>
<dl>
<dt><strong><code>dim_range</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>dict, with keys corresponding to channel/dimension labels as above
and slice objects as values. This allows you to only load part of
the data along any of the dimensions, such as only loading two
time steps or a particular z-range. An example use for only taking
time steps up to 5 and z-slice 20 to 30 would
be:
dim_range={'time':slice(None,5), 'z-axis':slice(20,30)}.
The default is {} which corresponds to the full file.</dd>
<dt><strong><code>dtype</code></strong> :&ensp;(<code>numpy</code>) <code>datatype</code>, optional</dt>
<dd>type to scale data to. The default is np.uint16.</dd>
<dt><strong><code>remove_backsteps</code></strong> :&ensp;<code>bool</code></dt>
<dd>whether to discard the frames which were recorded on the backsteps
downwards</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>zstack</code></strong> :&ensp;<code>iterable</code>/<code>generator</code> <code>yielding</code> <code>numpy.ndarray</code></dt>
<dd>list of time steps, with for each time step a z-stack as np.ndarray
with the pixel values</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def yield_stack(self,dim_range={},dtype=np.uint16,remove_backsteps=True):
    &#34;&#34;&#34;
    Lazy-load the data and reshape into 4D stack with the following
    dimension order: (&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;). Returns a
    generator which yields a z-stack for each call, which is loaded upon
    calling it.
    
    For loading only part of the total dataset, the dim_range parameter can
    be used to specify a range along any of the dimensions. This will be
    more memory efficient than loading the entire stack and then discarding
    part of the data. For slicing along the x or y axis this is not
    possible and whole (xy) images must be loaded prior to discarding
    data outside the specified x or y axis range.
    The shape of the stack can be accessed without loading data using the 
    stack_shape attribute after creating the yield_stack object.
    Parameters
    ----------
    dim_range : dict, optional
        dict, with keys corresponding to channel/dimension labels as above
        and slice objects as values. This allows you to only load part of
        the data along any of the dimensions, such as only loading two
        time steps or a particular z-range. An example use for only taking
        time steps up to 5 and z-slice 20 to 30 would
        be:
            dim_range={&#39;time&#39;:slice(None,5), &#39;z-axis&#39;:slice(20,30)}.
        The default is {} which corresponds to the full file.
    dtype : (numpy) datatype, optional
        type to scale data to. The default is np.uint16.
    remove_backsteps : bool
        whether to discard the frames which were recorded on the backsteps
        downwards
    Returns
    -------
    zstack : iterable/generator yielding numpy.ndarray
        list of time steps, with for each time step a z-stack as np.ndarray
        with the pixel values
    &#34;&#34;&#34;
    indices = np.reshape(range(self.nf),(self.nt,self.nz+self.backsteps))

    #remove backsteps from indices
    if remove_backsteps:
        indices = indices[:,:self.nz]

    #check dim_range items for faulty values
    for key in dim_range.keys():
        if type(key) != str or key not in [&#39;time&#39;,&#39;z-axis&#39;,&#39;y-axis&#39;,&#39;x-axis&#39;]:
            print(&#34;[WARNING] confocal.visitech_faststack.load_stack: &#34;+
                      &#34;dimension &#39;&#34;+key+&#34;&#39; not present in data, ignoring &#34;+
                      &#34;this entry.&#34;)
            dim_range.pop(key)

    #warn for inefficient x and y trimming
    if &#39;x-axis&#39; in dim_range or &#39;y-axis&#39; in dim_range:
        print(&#34;[WARNING] confocal.visitech_faststack.load_stack: Loading&#34;+
              &#34; only part of the data along dimensions &#39;x-axis&#39; and/or &#34;+
              &#34;&#39;y-axis&#39; not implemented. Data will be loaded fully &#34;+
              &#34;into memory before discarding values outside of the &#34;+
              &#34;slice range specified for the x-axis and/or y-axis. &#34;+
              &#34;Other axes for which a range is specified will still &#34;+
              &#34;be treated normally, avoiding unneccesary memory use.&#34;)

    #remove values outside of dim_range from indices
    if &#39;time&#39; in dim_range:
        indices = indices[dim_range[&#39;time&#39;]]
    if &#39;z-axis&#39; in dim_range:
        #assure backsteps cannot be removed this way
        if remove_backsteps:
            indices = indices[:,dim_range[&#39;z-axis&#39;]]
        else:
            backsteps = indices[:,self.nz:]
            indices = indices[:,dim_range[&#39;z-axis&#39;]]
            indices = np.concatenate((indices,backsteps),axis=1)

    #store image indices array for self.get_timestamps(load_stack_indices=True)
    self._stack_indices = indices

    #store stack size as attribute
    self.stack_shape = indices.shape + self.datafile[0].shape
    if &#39;y-axis&#39; in dim_range:
        self.stack_shape = self.stack_shape[:2] + (len(range(self.stack_shape[2])[dim_range[&#39;y-axis&#39;]]),self.stack_shape[3])
    if &#39;x-axis&#39; in dim_range:
        self.stack_shape = self.stack_shape[:3] + (len(range(self.stack_shape[3])[dim_range[&#39;x-axis&#39;]]),)

    #generator loop over each time step in a inner function such that the
    #initialization is excecuted up to this point upon creation rather than
    #upon iteration over the loop
    def stack_iter():
        for zstack_indices in indices:
            zstack = self.load_data(indices=zstack_indices.ravel(),dtype=dtype)
            zstack = zstack.reshape(self.stack_shape[1:])

            #trim x and y axis
            if &#39;y-axis&#39; in dim_range:
                zstack = zstack[:,dim_range[&#39;y-axis&#39;]]
            if &#39;x-axis&#39; in dim_range:
                zstack = zstack[:,:,dim_range[&#39;x-axis&#39;]]

            yield zstack

    return stack_iter()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="scm_confocal.sp8" href="sp8.html">scm_confocal.sp8</a></code></li>
<li><code><a title="scm_confocal.visitech" href="visitech.html">scm_confocal.visitech</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="scm_confocal.sp8_series" href="#scm_confocal.sp8_series">sp8_series</a></code></h4>
<ul class="">
<li><code><a title="scm_confocal.sp8_series.get_dimension_steps" href="#scm_confocal.sp8_series.get_dimension_steps">get_dimension_steps</a></code></li>
<li><code><a title="scm_confocal.sp8_series.get_dimension_stepsize" href="#scm_confocal.sp8_series.get_dimension_stepsize">get_dimension_stepsize</a></code></li>
<li><code><a title="scm_confocal.sp8_series.get_metadata_channels" href="#scm_confocal.sp8_series.get_metadata_channels">get_metadata_channels</a></code></li>
<li><code><a title="scm_confocal.sp8_series.get_metadata_dimension" href="#scm_confocal.sp8_series.get_metadata_dimension">get_metadata_dimension</a></code></li>
<li><code><a title="scm_confocal.sp8_series.get_metadata_dimensions" href="#scm_confocal.sp8_series.get_metadata_dimensions">get_metadata_dimensions</a></code></li>
<li><code><a title="scm_confocal.sp8_series.get_series_name" href="#scm_confocal.sp8_series.get_series_name">get_series_name</a></code></li>
<li><code><a title="scm_confocal.sp8_series.load_data" href="#scm_confocal.sp8_series.load_data">load_data</a></code></li>
<li><code><a title="scm_confocal.sp8_series.load_metadata" href="#scm_confocal.sp8_series.load_metadata">load_metadata</a></code></li>
<li><code><a title="scm_confocal.sp8_series.load_stack" href="#scm_confocal.sp8_series.load_stack">load_stack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scm_confocal.util" href="#scm_confocal.util">util</a></code></h4>
<ul class="">
<li><code><a title="scm_confocal.util.bin_stack" href="#scm_confocal.util.bin_stack">bin_stack</a></code></li>
<li><code><a title="scm_confocal.util.fit_powerlaw" href="#scm_confocal.util.fit_powerlaw">fit_powerlaw</a></code></li>
<li><code><a title="scm_confocal.util.mean_square_displacement" href="#scm_confocal.util.mean_square_displacement">mean_square_displacement</a></code></li>
<li><code><a title="scm_confocal.util.mean_square_displacement_per_frame" href="#scm_confocal.util.mean_square_displacement_per_frame">mean_square_displacement_per_frame</a></code></li>
<li><code><a title="scm_confocal.util.multiply_intensity" href="#scm_confocal.util.multiply_intensity">multiply_intensity</a></code></li>
<li><code><a title="scm_confocal.util.plot_stack_histogram" href="#scm_confocal.util.plot_stack_histogram">plot_stack_histogram</a></code></li>
<li><code><a title="scm_confocal.util.saveprompt" href="#scm_confocal.util.saveprompt">saveprompt</a></code></li>
<li><code><a title="scm_confocal.util.subtract_background" href="#scm_confocal.util.subtract_background">subtract_background</a></code></li>
<li><code><a title="scm_confocal.util.write_textfile" href="#scm_confocal.util.write_textfile">write_textfile</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scm_confocal.visitech_faststack" href="#scm_confocal.visitech_faststack">visitech_faststack</a></code></h4>
<ul class="two-column">
<li><code><a title="scm_confocal.visitech_faststack.get_metadata" href="#scm_confocal.visitech_faststack.get_metadata">get_metadata</a></code></li>
<li><code><a title="scm_confocal.visitech_faststack.get_pixelsize" href="#scm_confocal.visitech_faststack.get_pixelsize">get_pixelsize</a></code></li>
<li><code><a title="scm_confocal.visitech_faststack.get_timestamps" href="#scm_confocal.visitech_faststack.get_timestamps">get_timestamps</a></code></li>
<li><code><a title="scm_confocal.visitech_faststack.load_data" href="#scm_confocal.visitech_faststack.load_data">load_data</a></code></li>
<li><code><a title="scm_confocal.visitech_faststack.load_stack" href="#scm_confocal.visitech_faststack.load_stack">load_stack</a></code></li>
<li><code><a title="scm_confocal.visitech_faststack.save_stack" href="#scm_confocal.visitech_faststack.save_stack">save_stack</a></code></li>
<li><code><a title="scm_confocal.visitech_faststack.yield_stack" href="#scm_confocal.visitech_faststack.yield_stack">yield_stack</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="scm_confocal.visitech_series" href="#scm_confocal.visitech_series">visitech_series</a></code></h4>
<ul class="">
<li><code><a title="scm_confocal.visitech_series.get_dimension_steps" href="#scm_confocal.visitech_series.get_dimension_steps">get_dimension_steps</a></code></li>
<li><code><a title="scm_confocal.visitech_series.get_image_metadata" href="#scm_confocal.visitech_series.get_image_metadata">get_image_metadata</a></code></li>
<li><code><a title="scm_confocal.visitech_series.get_metadata" href="#scm_confocal.visitech_series.get_metadata">get_metadata</a></code></li>
<li><code><a title="scm_confocal.visitech_series.get_metadata_dimensions" href="#scm_confocal.visitech_series.get_metadata_dimensions">get_metadata_dimensions</a></code></li>
<li><code><a title="scm_confocal.visitech_series.get_pixelsize" href="#scm_confocal.visitech_series.get_pixelsize">get_pixelsize</a></code></li>
<li><code><a title="scm_confocal.visitech_series.load_data" href="#scm_confocal.visitech_series.load_data">load_data</a></code></li>
<li><code><a title="scm_confocal.visitech_series.load_stack" href="#scm_confocal.visitech_series.load_stack">load_stack</a></code></li>
<li><code><a title="scm_confocal.visitech_series.yield_stack" href="#scm_confocal.visitech_series.yield_stack">yield_stack</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>
